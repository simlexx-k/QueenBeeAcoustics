{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12134294,
          "sourceType": "datasetVersion",
          "datasetId": 7505074
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Queen Bee detection using CNN",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simlexx-k/QueenBeeAcoustics/blob/main/Queen_Bee_detection_using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "harshkumar1711_beehive_audio_dataset_with_queen_and_without_queen_path = kagglehub.dataset_download('harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "e4_RBIw_4X10",
        "outputId": "86713ded-c4ec-4b1d-ad2c-f8adfc834a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen?dataset_version_number=7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.15G/4.15G [00:50<00:00, 89.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Pipeline for Queen Bee Acoustic Monitoring Using CNN\n",
        "\n",
        "A complete end-to-end prototype to detect **Queen Bee Presence** from **Hive audio recordings** using a Machine Learning pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Live Demo (CNN prototype is in progress)\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "Note:\n",
        "You can try the deployed model (Model used Random Forest Classifier in this prototype) and download a sample audio file to test it.\n",
        "\n",
        "</div>\n",
        "\n",
        "<span style=\"display: inline-block;\">\n",
        "  <a href=\"http://3.109.237.216\">\n",
        "    <img src=\"https://img.shields.io/badge/Live-Demo-blue\" alt=\"Live Demo\">\n",
        "  </a>\n",
        "</span>\n",
        "\n",
        "<span style=\"display: inline-block;\" >\n",
        "<a href=\"https://drive.google.com/drive/folders/1KFmHH304soDKXbLvlohWBJ-qVzudjTgZ?usp=sharing\">\n",
        "    <img src=\"https://img.shields.io/badge/Google%20Drive-Sample%20Data-blue?logo=google-drive&logoColor=white\" alt=\"Google Drive\">\n",
        "</a>\n",
        "</span>\n",
        "    \n",
        "<span style=\"display: inline-block;\">\n",
        "<a href=\"https://github.com/Harsh-1711/BuzzDetect\">\n",
        "  <img src=\"https://img.shields.io/badge/GitHub-View%20Repository-blue?logo=github\" alt=\"GitHub Repo\">\n",
        "</a>\n",
        "</span>\n",
        "<span style=\"display: inline-block;\">\n",
        "  <a href=\"https://drive.google.com/file/d/1XldGWp5G2Ecby9GLpalsgegvOgMsL_HJ/view?usp=sharing\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/Demo-Video-green?logo=google-drive\" alt=\"Demo Video\">\n",
        "  </a>\n",
        "</span>\n",
        "\n",
        "---\n",
        "\n",
        "This notebook demonstrates a complete pipeline for processing hive audio recordings and applying machine learning to detect the presence of the queen bee.\n",
        "It covers audio feature extraction, dataset preparation, model training, evaluation, and inference."
      ],
      "metadata": {
        "id": "jKnuukST4X2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Import necessary libraries for the Queen Bee Detection project.\n",
        "\n",
        "- os: For file and directory operations.\n",
        "- numpy: For numerical operations on arrays and matrices.\n",
        "- matplotlib & seaborn: For data visualization.\n",
        "- librosa: For audio processing and feature extraction.\n",
        "\"\"\"\n",
        "!pip install keras-tuner tensorflow\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import keras_tuner as kt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-13T09:04:43.22203Z",
          "iopub.execute_input": "2025-06-13T09:04:43.222358Z",
          "iopub.status.idle": "2025-06-13T09:04:43.227533Z",
          "shell.execute_reply.started": "2025-06-13T09:04:43.222337Z",
          "shell.execute_reply": "2025-06-13T09:04:43.226768Z"
        },
        "trusted": true,
        "id": "Tbue9gj54X2Q",
        "outputId": "38ea30a5-8978-46a1-8494-f78a47aa7e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.8-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (2.32.4)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.76.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (5.29.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.8-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.8 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data Loading and Exploration**\n",
        "#### üìå Objective:\n",
        "    Load the dataset dataset and plot a graph showing the total no. of dataset for each category.\n",
        "    \n",
        "<div class=\"alert alert-block alert-success\">\n",
        "    Dataset Source:\n",
        "</div>\n",
        "        \n",
        "  - This dataset was originally sourced from https://zenodo.org/records/2667806.\n",
        "  - It contains labeled audio recordings from beehives for Queen Bee detection.\n",
        "  - Modified Dataset - https://www.kaggle.com/datasets/harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen"
      ],
      "metadata": {
        "id": "u1qUEuCM4X2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dataset from kaggle\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-13T09:04:49.922122Z",
          "iopub.execute_input": "2025-06-13T09:04:49.922721Z",
          "iopub.status.idle": "2025-06-13T09:05:13.760153Z",
          "shell.execute_reply.started": "2025-06-13T09:04:49.922694Z",
          "shell.execute_reply": "2025-06-13T09:05:13.759365Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Qc-UQ-Xu4X2U"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load base path\n",
        "base_path = \"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Bee Hive Audios\"\n",
        "\n",
        "# Include spaces prbboperly using raw string or quotes\n",
        "present_path = os.path.join(base_path, \"QueenBee Present\")\n",
        "absent_path = os.path.join(base_path, \"QueenBee Absent\")\n",
        "external_path= '/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/External Noise'\n",
        "\n",
        "# Count .wav files\n",
        "present_count = len([f for f in os.listdir(present_path) if f.endswith('.wav')])\n",
        "absent_count = len([f for f in os.listdir(absent_path) if f.endswith('.wav')])\n",
        "external_count = len([f for f in os.listdir(external_path) if f.endswith('.wav')])\n",
        "# Print counts\n",
        "print(f\"QueenBee Present: {present_count} files\")\n",
        "print(f\"QueenBee Absent: {absent_count} files\")\n",
        "print(f\"External Noise: {external_count} files\")\n",
        "\n",
        "# Plot the counts\n",
        "labels = ['QueenBee Present', 'QueenBee Absent','External Noise']\n",
        "counts = [present_count, absent_count, external_count]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.ylim(0, 4300)\n",
        "bars=plt.bar(labels, counts, color=['brown', 'beige','orange'],edgecolor='black')\n",
        "plt.grid(axis=\"y\",linestyle=\"--\",alpha=0.7)\n",
        "for bar in bars:\n",
        "    x = bar.get_x() + bar.get_width()/2\n",
        "    y = bar.get_height()\n",
        "    plt.text(x, y+y ** 0.5, str(y), ha='center', va='bottom', fontsize=10, color='black')\n",
        "plt.title(\"Queen Bee Audio File Count\")\n",
        "plt.ylabel(\"Number of Files\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-13T09:05:15.286055Z",
          "iopub.execute_input": "2025-06-13T09:05:15.286612Z",
          "iopub.status.idle": "2025-06-13T09:05:15.699588Z",
          "shell.execute_reply.started": "2025-06-13T09:05:15.286578Z",
          "shell.execute_reply": "2025-06-13T09:05:15.698698Z"
        },
        "trusted": true,
        "id": "xUuQC_Oo4X2V",
        "outputId": "9a92b4f9-39bf-4a1e-9d65-8b43b043bd7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Bee Hive Audios/QueenBee Present'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-781139536.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Count .wav files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpresent_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mabsent_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsent_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mexternal_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Bee Hive Audios/QueenBee Present'"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Audio Preprocessing and Spectrogram Generation**\n",
        "#### üìå Objective:\n",
        "    This section defines parameters for audio sampling and a function to convert `.wav` audio files into mel spectrogram images. The function performs the following steps:\n",
        "    - Loads and trims the audio.\n",
        "    - Normalizes the signal and ensures a fixed length.\n",
        "    - Generates a mel spectrogram using Librosa.\n",
        "    - Saves the spectrogram as a `.png` image for use in model training.\n",
        "\n",
        "This is a key step in preparing audio data for convolutional neural networks (CNNs), which work better with visual inputs."
      ],
      "metadata": {
        "id": "88mp3Ahc4X2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_base = \"/kaggle/working/Dataset/spectrograms\"\n",
        "os.makedirs(os.path.join(output_base, \"present\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base, \"absent\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base, \"external\"), exist_ok=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-11T12:25:26.936475Z",
          "iopub.execute_input": "2025-06-11T12:25:26.936816Z",
          "iopub.status.idle": "2025-06-11T12:25:26.943065Z",
          "shell.execute_reply.started": "2025-06-11T12:25:26.936791Z",
          "shell.execute_reply": "2025-06-11T12:25:26.942159Z"
        },
        "trusted": true,
        "id": "sggglMUt4X2Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 22050\n",
        "DURATION = 3  # seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
        "\n",
        "# Function to process and save spectrograms\n",
        "def preprocess_and_save_spectrogram(audio_path, output_image_path, sr=SAMPLE_RATE, duration=DURATION):\n",
        "\n",
        "    \"\"\"\n",
        "    Load an audio file, convert it into a mel spectrogram, and save it as an image.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    audio_path : str\n",
        "        Path to the input .wav audio file.\n",
        "    output_image_path : str\n",
        "        Path where the output spectrogram image will be saved.\n",
        "    sr : int, optional\n",
        "        Sampling rate for the audio. Default is 22050 Hz.\n",
        "    duration : int, optional\n",
        "        Duration (in seconds) to which the audio will be fixed. Default is 3 seconds.\n",
        "\n",
        "    Steps:\n",
        "    ------\n",
        "    1. Load and trim silence from the audio.\n",
        "    2. Convert to mono and normalize the waveform.\n",
        "    3. Pad or truncate to fixed duration.\n",
        "    4. Generate a mel spectrogram.\n",
        "    5. Convert the power spectrogram to decibels.\n",
        "    6. Save the spectrogram as a `.png` image (suitable for CNN input).\n",
        "\n",
        "    Exceptions:\n",
        "    -----------\n",
        "    If any error occurs during processing, the function will print an error message.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        y, _ = librosa.load(audio_path, sr=sr)\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        y = librosa.to_mono(y) if y.ndim > 1 else y\n",
        "        y = librosa.util.normalize(y)\n",
        "\n",
        "        max_len = sr * duration\n",
        "        if len(y) > max_len:\n",
        "            y = y[:max_len]\n",
        "        else:\n",
        "            y = np.pad(y, (0, max_len - len(y)))\n",
        "\n",
        "        # Create mel spectrogram\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        # Save as image\n",
        "        plt.figure(figsize=(2.56, 2.56), dpi=100)\n",
        "        librosa.display.specshow(S_dB, sr=sr, cmap='magma')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout(pad=0)\n",
        "        plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-11T12:25:29.360153Z",
          "iopub.execute_input": "2025-06-11T12:25:29.360435Z",
          "iopub.status.idle": "2025-06-11T12:25:29.369162Z",
          "shell.execute_reply.started": "2025-06-11T12:25:29.360416Z",
          "shell.execute_reply": "2025-06-11T12:25:29.368039Z"
        },
        "trusted": true,
        "id": "JXeFWqI24X2a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in tqdm(os.listdir(present_path), desc=\"Processing Present\"):\n",
        "    if filename.endswith('.wav'):\n",
        "        input_path = os.path.join(present_path, filename)\n",
        "        output_path = os.path.join(output_base, \"present\", filename.replace(\".wav\", \".png\"))\n",
        "        preprocess_and_save_spectrogram(input_path, output_path)\n",
        "\n",
        "# Process Absent Files\n",
        "for filename in tqdm(os.listdir(absent_path), desc=\"Processing Absent\"):\n",
        "    if filename.endswith('.wav'):\n",
        "        input_path = os.path.join(absent_path, filename)\n",
        "        output_path = os.path.join(output_base, \"absent\", filename.replace(\".wav\", \".png\"))\n",
        "        preprocess_and_save_spectrogram(input_path, output_path)\n",
        "\n",
        "# Process External Noise Files\n",
        "for root, dirs, files in os.walk(external_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith('.wav'):\n",
        "            input_path = os.path.join(root, filename)\n",
        "            output_path = os.path.join(output_base, \"external\", filename.replace(\".wav\", \".png\"))\n",
        "            preprocess_and_save_spectrogram(input_path, output_path)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Spectrogram image generation complete.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-11T12:25:34.794321Z",
          "iopub.execute_input": "2025-06-11T12:25:34.794605Z",
          "iopub.status.idle": "2025-06-11T12:41:02.668411Z",
          "shell.execute_reply.started": "2025-06-11T12:25:34.794586Z",
          "shell.execute_reply": "2025-06-11T12:41:02.667192Z"
        },
        "trusted": true,
        "id": "UTXOhlJJ4X2f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Visualizing Spectrogram Distribution Across Classes**\n",
        "\n",
        "This section analyzes the dataset by counting the number of spectrogram images in each class ‚Äî \"present\" and \"absent\" ‚Äî and visualizes the distribution using a bar chart.\n",
        "\n",
        "    Steps performed:\n",
        "    - Counts `.png` spectrogram files in each class directory.\n",
        "    - Stores and prints the counts.\n",
        "    - Plots a bar graph to visualize class balance.\n",
        "    - Adds numeric labels on top of each bar for clarity.\n",
        "\n",
        "This helps assess whether the dataset is balanced, which is crucial for model performance in classification tasks.\n"
      ],
      "metadata": {
        "id": "rP6wdcRe4X2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Spectrogram image path\n",
        "spectrogram_path = \"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Spectograms\"\n",
        "\n",
        "# Define class folders\n",
        "class_folders = ['present', 'absent']\n",
        "counts = []\n",
        "\n",
        "# Count PNG files in each folder\n",
        "for label in class_folders:\n",
        "    class_path = os.path.join(spectrogram_path, label)\n",
        "    count = len([f for f in os.listdir(class_path) if f.endswith('.png')])\n",
        "    counts.append(count)\n",
        "    print(f\"{label}: {count} spectrograms\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(class_folders, counts, color=['darkorange', 'skyblue'], edgecolor='black')\n",
        "plt.ylim(0,4400)\n",
        "plt.title(\"Number of Spectrogram PNGs per Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Add count labels on top\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval+yval ** 0.5, str(yval), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-13T09:06:13.263923Z",
          "iopub.execute_input": "2025-06-13T09:06:13.26425Z",
          "iopub.status.idle": "2025-06-13T09:06:13.417494Z",
          "shell.execute_reply.started": "2025-06-13T09:06:13.264227Z",
          "shell.execute_reply": "2025-06-13T09:06:13.416775Z"
        },
        "trusted": true,
        "id": "XHS1jgjY4X2g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Preparing Data Loaders: Train, Validation, and Test Splits**\n",
        "\n",
        "This section prepares the spectrogram dataset for training a CNN using TensorFlow's `ImageDataGenerator`.\n",
        "\n",
        "- **Base Directory**: Points to the location of spectrogram images organized by class.\n",
        "- **Image Preprocessing**: Rescales pixel values to the [0, 1] range.\n",
        "- **Splitting Strategy**:\n",
        "  - **70%** of the data is used for training.\n",
        "  - The remaining **30%** is further split evenly into **15% validation** and **15% testing**.\n",
        "- **Generators Created**:\n",
        "  - `train_gen` for training the model.\n",
        "  - `val_gen` for tuning hyperparameters and monitoring overfitting.\n",
        "  - `test_gen` for final performance evaluation.\n",
        "\n",
        "This setup ensures consistent and reproducible splits with a fixed seed.\n"
      ],
      "metadata": {
        "id": "ba-hdTP_4X2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base directory\n",
        "data_dir = \"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Spectograms\"\n",
        "\n",
        "# Params\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "# üíæ Data split: 70% train, 15% val, 15% test\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.30)\n",
        "\n",
        "# 70% Train\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# Split remaining 30% into 15% val + 15% test\n",
        "datagen_val_test = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "\n",
        "val_gen = datagen_val_test.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "test_gen = datagen_val_test.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=SEED,\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-11T16:49:28.20264Z",
          "iopub.execute_input": "2025-06-11T16:49:28.202944Z",
          "iopub.status.idle": "2025-06-11T16:49:29.67822Z",
          "shell.execute_reply.started": "2025-06-11T16:49:28.202921Z",
          "shell.execute_reply": "2025-06-11T16:49:29.677456Z"
        },
        "trusted": true,
        "id": "kXXpQ7qQ4X2l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = '/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Spectograms/Spectograms'\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "gen = datagen.flow_from_directory(Dataset, class_mode='binary')\n",
        "print(gen.class_indices)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-11T16:49:46.367186Z",
          "iopub.execute_input": "2025-06-11T16:49:46.367713Z",
          "iopub.status.idle": "2025-06-11T16:49:47.683966Z",
          "shell.execute_reply.started": "2025-06-11T16:49:46.367687Z",
          "shell.execute_reply": "2025-06-11T16:49:47.683433Z"
        },
        "id": "NVmqrZGw4X2l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Builds and compiles a convolutional neural network (CNN) model using hyperparameters\n",
        "    provided by Keras Tuner's HyperParameters object.\n",
        "\n",
        "    The model is designed for binary image classification (e.g., presence of queen bee).\n",
        "\n",
        "    Hyperparameters tuned:\n",
        "    - conv_1_filter: Number of filters in the first Conv2D layer (choices: 32, 64)\n",
        "    - conv_1_kernel: Kernel size for the first Conv2D layer (choices: 3, 5)\n",
        "    - conv_2_filter: Number of filters in the second Conv2D layer (choices: 64, 128)\n",
        "    - conv_2_kernel: Kernel size for the second Conv2D layer (choices: 3, 5)\n",
        "    - dense_units: Number of units in the Dense layer (range: 64 to 256, step: 32)\n",
        "    - dropout: Dropout rate after Dense layer (range: 0.2 to 0.5, step: 0.1)\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras.Model): Compiled CNN model.\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=hp.Choice('conv_1_filter', [32, 64]),\n",
        "        kernel_size=hp.Choice('conv_1_kernel', [3, 5]),\n",
        "        activation='relu',\n",
        "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
        "    ))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=hp.Choice('conv_2_filter', [64, 128]),\n",
        "        kernel_size=hp.Choice('conv_2_kernel', [3, 5]),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(\n",
        "        units=hp.Int('dense_units', min_value=64, max_value=256, step=32),\n",
        "        activation='relu'\n",
        "    ))\n",
        "    model.add(layers.Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1)))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Tuner setup\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    directory='queenbee_tuner',\n",
        "    project_name='queen_cnn'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# ‚è≥ Run tuning\n",
        "tuner.search(train_gen, validation_data=val_gen, epochs=10)\n",
        "\n",
        "# üèÜ Get best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-11T16:50:25.433361Z",
          "iopub.execute_input": "2025-06-11T16:50:25.434115Z",
          "iopub.status.idle": "2025-06-11T17:14:29.442443Z",
          "shell.execute_reply.started": "2025-06-11T16:50:25.434082Z",
          "shell.execute_reply": "2025-06-11T17:14:29.441838Z"
        },
        "id": "C0xnUUSF4X2m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train + val\n",
        "datagen_full = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
        "final_train_gen = datagen_full.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=SEED\n",
        ")\n",
        "final_val_gen = datagen_full.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# Train\n",
        "history = best_model.fit(\n",
        "    final_train_gen,\n",
        "    validation_data=final_val_gen,\n",
        "    epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-11T17:14:50.993682Z",
          "iopub.execute_input": "2025-06-11T17:14:50.994177Z",
          "iopub.status.idle": "2025-06-11T17:18:57.414155Z",
          "shell.execute_reply.started": "2025-06-11T17:14:50.994152Z",
          "shell.execute_reply": "2025-06-11T17:18:57.413551Z"
        },
        "trusted": true,
        "id": "i1IZG9fn4X2p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "best_model.save(\"/kaggle/working/queenbee_final_model.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-11T17:19:05.452212Z",
          "iopub.execute_input": "2025-06-11T17:19:05.452494Z",
          "iopub.status.idle": "2025-06-11T17:19:05.759479Z",
          "shell.execute_reply.started": "2025-06-11T17:19:05.452475Z",
          "shell.execute_reply": "2025-06-11T17:19:05.758941Z"
        },
        "trusted": true,
        "id": "LqTpWxQ-4X2r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#load the fine-tuned model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/queenbee_final_model.h5\") # change the path if you are using your own trained model\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-13T09:07:25.980045Z",
          "iopub.execute_input": "2025-06-13T09:07:25.980669Z",
          "iopub.status.idle": "2025-06-13T09:07:26.385779Z",
          "shell.execute_reply.started": "2025-06-13T09:07:25.980645Z",
          "shell.execute_reply": "2025-06-13T09:07:26.385049Z"
        },
        "trusted": true,
        "id": "UeddtSHb4X2r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.** **Evaluate Metrics**\n",
        "\n",
        "* Accuracy, Precision, Recall, F1, ROC AUC  \n",
        "* Confusion Matrix  \n",
        "* ROC & PR Curves  \n",
        "* Test Set Performance Summary"
      ],
      "metadata": {
        "id": "4AcyjPg44X2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(model, test_gen):\n",
        "    \"\"\"\n",
        "    Runs inference on the given test data generator using a trained model.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Trained Keras model for binary classification.\n",
        "        test_gen (ImageDataGenerator): Test data generator with images and labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            y_pred (np.ndarray): Binary class predictions (0 or 1).\n",
        "            y_prob (np.ndarray): Predicted probabilities for the positive class.\n",
        "            y_true (np.ndarray): Ground truth labels from the test generator.\n",
        "    \"\"\"\n",
        "    test_gen.reset()\n",
        "    y_prob = model.predict(test_gen, steps=int(np.ceil(test_gen.samples / test_gen.batch_size))).ravel()\n",
        "    y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "    return y_pred, y_prob, test_gen.classes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-13T09:07:35.421869Z",
          "iopub.execute_input": "2025-06-13T09:07:35.422475Z",
          "iopub.status.idle": "2025-06-13T09:07:35.426977Z",
          "shell.execute_reply.started": "2025-06-13T09:07:35.422452Z",
          "shell.execute_reply": "2025-06-13T09:07:35.426179Z"
        },
        "id": "_ieP3tZk4X2r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    classification_report, roc_curve, precision_recall_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# üìà Metrics\n",
        "y_pred, y_prob, y_true = run_inference(model, test_gen)\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec = precision_score(y_true, y_pred)\n",
        "rec = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_prob)\n",
        "pr_auc = average_precision_score(y_true, y_prob)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"AU-ROC:    {roc_auc:.4f}\")\n",
        "print(f\"AU-PRC:    {pr_auc:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-13T09:07:43.538517Z",
          "iopub.execute_input": "2025-06-13T09:07:43.539159Z",
          "iopub.status.idle": "2025-06-13T09:07:44.001096Z",
          "shell.execute_reply.started": "2025-06-13T09:07:43.539137Z",
          "shell.execute_reply": "2025-06-13T09:07:44.000096Z"
        },
        "id": "sgweG7Ww4X2s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=test_gen.class_indices.keys(), yticklabels=test_gen.class_indices.keys())\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "#ROC curve\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"AU-ROC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#precision recall curve\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=f\"AU-PRC = {pr_auc:.4f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#Histogram of the predicted model\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(y_prob[y_true == 0], bins=25, alpha=0.7, label='Class 0')\n",
        "plt.hist(y_prob[y_true == 1], bins=25, alpha=0.7, label='Class 1')\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Predicted Probabilities by True Class\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-13T09:08:02.363495Z",
          "iopub.execute_input": "2025-06-13T09:08:02.364359Z",
          "iopub.status.idle": "2025-06-13T09:08:02.382588Z",
          "shell.execute_reply.started": "2025-06-13T09:08:02.364329Z",
          "shell.execute_reply": "2025-06-13T09:08:02.381598Z"
        },
        "id": "FCovrxY24X2s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred, target_names=list(test_gen.class_indices.keys())))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-11T17:26:58.257988Z",
          "iopub.execute_input": "2025-06-11T17:26:58.258287Z",
          "iopub.status.idle": "2025-06-11T17:26:58.269745Z",
          "shell.execute_reply.started": "2025-06-11T17:26:58.25827Z",
          "shell.execute_reply": "2025-06-11T17:26:58.269174Z"
        },
        "id": "ceZF-KLA4X2u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation Commentary\n",
        "The model demonstrates excellent performance with an **overall accuracy of 99%**. Both classes ‚Äî \"absent\" and \"present\" ‚Äî have near-perfect **precision, recall, and F1-scores (0.99)**, indicating:\n",
        "\n",
        "**High precision**: Very few false positives.\n",
        "\n",
        "**High recall**: Nearly all relevant instances were correctly identified.\n",
        "\n",
        "**Balanced performance**: Consistent metrics across both classes, confirmed by the macro and weighted averages."
      ],
      "metadata": {
        "id": "iyNwUsUZ4X2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Prediction and Audio Feature Visualization**\n",
        "### üìå Objective:\n",
        "Visualize key audio features from a sound signal to better understand the characteristics of the data.\n",
        "\n",
        "#### Features Plotted:\n",
        "    - Raw audio waveform\n",
        "    - Mel-spectrogram input to CNN\n",
        "    - Model prediction probabilities (Queen Present/Absent)\n",
        "    - Audio statistics (duration, RMS energy, zero-crossing rate, etc.)"
      ],
      "metadata": {
        "id": "RiYBolAH4X2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_audio_prediction(audio_path, model):\n",
        "    \"\"\"\n",
        "    Create essential visualizations for audio CNN prediction\n",
        "    \"\"\"\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(audio_path, sr=SR)\n",
        "\n",
        "    # Create mel-spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Get model input (spectrogram image)\n",
        "    input_data = audio_to_spectrogram_image(audio_path, model)\n",
        "\n",
        "    # Get prediction\n",
        "    prediction = model.predict(input_data)\n",
        "    queen_prob = float(prediction[0][0])\n",
        "    label = \"Queen Present\" if queen_prob >= 0.5 else \"Queen Absent\"\n",
        "    confidence = queen_prob if queen_prob >= 0.5 else (1 - queen_prob)\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(f'Audio Analysis: {audio_path.split(\"/\")[-1]}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Raw Waveform\n",
        "    axes[0, 0].plot(np.linspace(0, len(y)/sr, len(y)), y, color='blue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Raw Audio Waveform', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Time (seconds)')\n",
        "    axes[0, 0].set_ylabel('Amplitude')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Mel-Spectrogram (what CNN sees)\n",
        "    im = axes[0, 1].imshow(S_dB, aspect='auto', origin='lower', cmap='magma')\n",
        "    axes[0, 1].set_title('Mel-Spectrogram (CNN Input)', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Time Frames')\n",
        "    axes[0, 1].set_ylabel('Mel Frequency Bins')\n",
        "    plt.colorbar(im, ax=axes[0, 1], label='Power (dB)')\n",
        "\n",
        "    # 3. Prediction Results\n",
        "    categories = ['Queen Absent', 'Queen Present']\n",
        "    probabilities = [1 - queen_prob, queen_prob]\n",
        "    colors = ['red' if p == max(probabilities) else 'lightcoral' for p in probabilities]\n",
        "\n",
        "    bars = axes[1, 0].bar(categories, probabilities, color=colors, alpha=0.8, edgecolor='black')\n",
        "    axes[1, 0].set_title('Model Predictions', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Probability')\n",
        "    axes[1, 0].set_ylim(0, 1)\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add probability labels on bars\n",
        "    for bar, prob in zip(bars, probabilities):\n",
        "        height = bar.get_height()\n",
        "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                       f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # 4. Audio Statistics\n",
        "    axes[1, 1].axis('off')\n",
        "    duration = len(y) / sr\n",
        "    max_amplitude = np.max(np.abs(y))\n",
        "    rms_energy = np.sqrt(np.mean(y**2))\n",
        "    zero_crossings = np.sum(np.diff(np.sign(y)) != 0) / len(y)\n",
        "\n",
        "    stats_text = f\"\"\"\n",
        "    Audio Statistics:\n",
        "\n",
        "    Duration: {duration:.2f} seconds\n",
        "    Sample Rate: {sr} Hz\n",
        "    Max Amplitude: {max_amplitude:.4f}\n",
        "    RMS Energy: {rms_energy:.4f}\n",
        "    Zero Crossing Rate: {zero_crossings:.4f}\n",
        "\n",
        "    Prediction Results:\n",
        "\n",
        "    Final Prediction: {label}\n",
        "    Confidence: {confidence*100:.2f}%\n",
        "\n",
        "    Queen Present Prob: {queen_prob:.3f}\n",
        "    Queen Absent Prob: {1-queen_prob:.3f}\n",
        "    \"\"\"\n",
        "\n",
        "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes,\n",
        "                   fontsize=12, verticalalignment='top',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'prediction': label,\n",
        "        'confidence': confidence,\n",
        "        'queen_probability': queen_prob,\n",
        "        'duration': duration,\n",
        "        'audio_stats': {\n",
        "            'max_amplitude': max_amplitude,\n",
        "            'rms_energy': rms_energy,\n",
        "            'zero_crossing_rate': zero_crossings\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-13T09:08:24.204762Z",
          "iopub.execute_input": "2025-06-13T09:08:24.205048Z",
          "iopub.status.idle": "2025-06-13T09:08:24.217774Z",
          "shell.execute_reply.started": "2025-06-13T09:08:24.205028Z",
          "shell.execute_reply": "2025-06-13T09:08:24.21708Z"
        },
        "id": "qqwFwaGb4X2v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# --- Parameters (must match training setup) ---\n",
        "IMG_SIZE = (128, 128)  # your model's input size\n",
        "SR = 22050  # sampling rate used during training\n",
        "\n",
        "# --- Function to process a .wav file into spectrogram image ---\n",
        "def audio_to_spectrogram_image(audio_path,model=None):\n",
        "    y, sr = librosa.load(audio_path, sr=SR)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Plot to image in memory\n",
        "    fig = plt.figure(figsize=(2, 2), dpi=64)  # ~128x128\n",
        "    librosa.display.specshow(S_dB, sr=sr, cmap='magma')\n",
        "    plt.axis('off')\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Load image and resize\n",
        "    img = Image.open(buf).convert('RGB').resize(IMG_SIZE)\n",
        "    img_array = np.array(img) / 255.0  # Normalize like training\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Shape: (1, 128, 128, 3)\n",
        "    return img_array\n",
        "\n",
        "# --- Predict ---\n",
        "audio_file_path = \"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Bee Hive Audios/QueenBee Absent/Hive1 31_05_2018_NO_QueenBee____00_00_00_chunk1.wav\"\n",
        "# run_inference(model,test_gen)\n",
        "input_data = audio_to_spectrogram_image(audio_file_path)\n",
        "\n",
        "results = visualize_audio_prediction(audio_file_path, model)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"PREDICTION SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"File: {audio_file_path.split('/')[-1]}\")\n",
        "print(f\"Prediction: {results['prediction']}\")\n",
        "print(f\"Confidence: {results['confidence']*100:.2f}%\")\n",
        "print(f\"Audio Duration: {results['duration']:.2f} seconds\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "audio_file_path = \"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset/Bee Hive Audios/QueenBee Present/Hive1 12_06_2018_QueenBee____00_00_00_chunk1.wav\"\n",
        "# run_inference(model,test_gen)\n",
        "input_data = audio_to_spectrogram_image(audio_file_path)\n",
        "\n",
        "results = visualize_audio_prediction(audio_file_path, model)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"PREDICTION SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"File: {audio_file_path.split('/')[-1]}\")\n",
        "print(f\"Prediction: {results['prediction']}\")\n",
        "print(f\"Confidence: {results['confidence']*100:.2f}%\")\n",
        "print(f\"Audio Duration: {results['duration']:.2f} seconds\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-06-13T09:09:02.726916Z",
          "iopub.execute_input": "2025-06-13T09:09:02.727504Z",
          "iopub.status.idle": "2025-06-13T09:09:23.914004Z",
          "shell.execute_reply.started": "2025-06-13T09:09:02.72748Z",
          "shell.execute_reply": "2025-06-13T09:09:23.913127Z"
        },
        "trusted": true,
        "id": "Y0a38Kbk4X2v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "iS0mxpVg4X2w"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}