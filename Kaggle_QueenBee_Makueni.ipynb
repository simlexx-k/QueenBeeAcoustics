{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 12134294,
     "sourceType": "datasetVersion",
     "datasetId": 7505074
    }
   ],
   "dockerImageVersionId": 31234,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "fdc48dcf",
   "cell_type": "markdown",
   "source": "# Kaggle Cloud Ops: Queen Bee Acoustics + Makueni Apiary Intelligence",
   "metadata": {}
  },
  {
   "id": "9325b117",
   "cell_type": "markdown",
   "source": "This unified notebook stitches together:\n\n1. **Queen Bee acoustic detection (CNN + hyperparameter tuning)**\n2. **Makueni Apiary intelligence workflows (weather, NDVI, telemetry, hive stress ML)**\n\n> **Kaggle usage:** Attach the `harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen` dataset plus any `content/main-data` exports as Kaggle data sources. All intermediate files are written under `content/` so the same notebook also works locally.",
   "metadata": {}
  },
  {
   "id": "ffc483a3",
   "cell_type": "code",
   "source": "!pip install -q earthengine-api ipyleaflet ipywidgets keras-tuner librosa tqdm",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:11:20.068238Z",
     "iopub.execute_input": "2025-12-23T09:11:20.069152Z",
     "iopub.status.idle": "2025-12-23T09:11:23.574245Z",
     "shell.execute_reply.started": "2025-12-23T09:11:20.069116Z",
     "shell.execute_reply": "2025-12-23T09:11:23.573427Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "id": "422e9772",
   "cell_type": "code",
   "source": "import os\nimport shutil\nfrom pathlib import Path\n\nPROJECT_ROOT = Path.cwd()\nDEFAULT_CONTENT = PROJECT_ROOT / \"content\"\nKAGGLE_WORKING = Path(\"/kaggle/working\")\n\nif DEFAULT_CONTENT.exists():\n    CONTENT_ROOT = DEFAULT_CONTENT.resolve()\nelse:\n    CONTENT_ROOT = (KAGGLE_WORKING / \"content\").resolve()\n    CONTENT_ROOT.mkdir(parents=True, exist_ok=True)\n\nos.environ[\"MERGED_CONTENT_ROOT\"] = str(CONTENT_ROOT)\nMAIN_DATA_DIR = (CONTENT_ROOT / \"main-data\")\nMAIN_DATA_DIR.mkdir(parents=True, exist_ok=True)\n\nKAGGLE_INPUT_ROOT = Path(\"/kaggle/input\")\n\ndef _stage_dataset(keyword, target_subdir):\n    if not KAGGLE_INPUT_ROOT.exists():\n        return None\n    matches = [p for p in KAGGLE_INPUT_ROOT.iterdir() if keyword in p.name.lower()]\n    if not matches:\n        print(f\"[setup] Kaggle input dataset containing '{keyword}' not found.\")\n        return None\n    source = matches[0]\n    target = CONTENT_ROOT / target_subdir\n    shutil.rmtree(target, ignore_errors=True)\n    shutil.copytree(source, target, dirs_exist_ok=True)\n    print(f\"[setup] Staged {source.name} -> {target}\")\n    return target\n\ndef _maybe_stage(keyword, subdir):\n    try:\n        _stage_dataset(keyword, subdir)\n    except Exception as exc:\n        print(f\"[setup] Skipping auto-stage for {keyword}: {exc}\")\n\n_maybe_stage(\"beehive\", \"beehive_audio\")\n_maybe_stage(\"makueni\", \"main-data\")\n\nprint(f\"CONTENT_ROOT -> {CONTENT_ROOT}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:11:39.528197Z",
     "iopub.execute_input": "2025-12-23T09:11:39.529035Z",
     "iopub.status.idle": "2025-12-23T09:12:45.459079Z",
     "shell.execute_reply.started": "2025-12-23T09:11:39.528999Z",
     "shell.execute_reply": "2025-12-23T09:12:45.458437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[setup] Staged beehive-audio-dataset-with-queen-and-without-queen -> /kaggle/working/content/beehive_audio\n[setup] Kaggle input dataset containing 'makueni' not found.\nCONTENT_ROOT -> /kaggle/working/content\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "id": "e8446e4b",
   "cell_type": "code",
   "source": "import calendar\nimport datetime as dt\nimport gc\nimport io\nimport json\nimport math\nimport os\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport keras_tuner as kt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    average_precision_score,\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    precision_recall_curve,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n    RocCurveDisplay\n)\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport requests\n\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 4)\n\nCONTENT_ROOT = Path(os.environ[\"MERGED_CONTENT_ROOT\"])\nMAIN_DATA_DIR = CONTENT_ROOT / \"main-data\"\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:13:28.802155Z",
     "iopub.execute_input": "2025-12-23T09:13:28.802481Z",
     "iopub.status.idle": "2025-12-23T09:13:34.523039Z",
     "shell.execute_reply.started": "2025-12-23T09:13:28.802453Z",
     "shell.execute_reply": "2025-12-23T09:13:34.522067Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-12-23 09:13:30.033031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766481210.054537    4466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766481210.061049    4466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766481210.078566    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766481210.078601    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766481210.078603    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766481210.078605    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 19
  },
  {
   "id": "00311da8",
   "cell_type": "markdown",
   "source": "## Queen Bee Acoustic Detection Pipeline",
   "metadata": {}
  },
  {
   "id": "1c46188a",
   "cell_type": "code",
   "source": "from pathlib import Path\n\ndef _discover_audio_dataset(content_root: Path) -> Path:\n    search_root = Path(\"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen\")\n    if not search_root.exists():\n        raise FileNotFoundError(\n            \"Dataset not staged. Attach Kaggle dataset \"\n            \"'harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen'.\"\n        )\n\n    for candidate in sorted(search_root.rglob(\"Dataset\")):\n        if (candidate / \"Bee Hive Audios\").exists():\n            return candidate\n\n    raise FileNotFoundError(\"Could not locate 'Dataset/Bee Hive Audios'.\")\n\n# Discover dataset (READ-ONLY)\nAUDIO_DATASET_ROOT = _discover_audio_dataset(None)\n\nBEEHIVE_AUDIO_DIR = next(AUDIO_DATASET_ROOT.glob(\"**/Bee Hive Audios\"))\nQUEEN_PRESENT_DIR = BEEHIVE_AUDIO_DIR / \"QueenBee Present\"\nQUEEN_ABSENT_DIR = BEEHIVE_AUDIO_DIR / \"QueenBee Absent\"\nEXTERNAL_DIR = AUDIO_DATASET_ROOT / \"External Noise\"\n\n# WRITEABLE spectrogram directory\nSPECTROGRAM_DIR = Path(\"/kaggle/working/spectrograms\")\nSPECTROGRAM_PRESENT = SPECTROGRAM_DIR / \"present\"\nSPECTROGRAM_ABSENT = SPECTROGRAM_DIR / \"absent\"\nSPECTROGRAM_EXTERNAL = SPECTROGRAM_DIR / \"external\"\n\nfor path in [SPECTROGRAM_PRESENT, SPECTROGRAM_ABSENT, SPECTROGRAM_EXTERNAL]:\n    path.mkdir(parents=True, exist_ok=True)\n\nprint(\"Audio dataset root (read-only):\", AUDIO_DATASET_ROOT)\nprint(\"Spectrogram cache (writable):\", SPECTROGRAM_DIR)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:13:41.060059Z",
     "iopub.execute_input": "2025-12-23T09:13:41.060652Z",
     "iopub.status.idle": "2025-12-23T09:13:49.624730Z",
     "shell.execute_reply.started": "2025-12-23T09:13:41.060620Z",
     "shell.execute_reply": "2025-12-23T09:13:49.623932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Audio dataset root (read-only): /kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset\nSpectrogram cache (writable): /kaggle/working/spectrograms\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 20
  },
  {
   "id": "5601ca8a",
   "cell_type": "code",
   "source": [
    "try:\n",
    "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
    "    ACCELERATOR = \"TPU\"\n",
    "except (ValueError, tf.errors.NotFoundError):\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Default to single-replica strategy for Kaggle GPU stability\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        ACCELERATOR = \"GPU\"\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        ACCELERATOR = \"CPU\"\n",
    "\n",
    "print(f\"Using {ACCELERATOR} via {strategy.__class__.__name__}\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:13:58.526186Z",
     "iopub.execute_input": "2025-12-23T09:13:58.526704Z",
     "iopub.status.idle": "2025-12-23T09:13:58.862719Z",
     "shell.execute_reply.started": "2025-12-23T09:13:58.526678Z",
     "shell.execute_reply": "2025-12-23T09:13:58.861969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\nUsing 2xGPU via MirroredStrategy\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "I0000 00:00:1766481238.825224    4466 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1766481238.829195    4466 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 21
  },
  {
   "id": "20b2348e",
   "cell_type": "code",
   "source": "SAMPLE_RATE = 22050\nDURATION = 3\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n\nlibrosa.cache.clear()\nplt.switch_backend(\"Agg\")\n\ndef preprocess_and_save_spectrogram(audio_path: Path, output_image_path: Path, sr=SAMPLE_RATE, duration=DURATION):\n    try:\n        y, _ = librosa.load(audio_path, sr=sr)\n        y, _ = librosa.effects.trim(y)\n        y = librosa.to_mono(y) if y.ndim > 1 else y\n        y = librosa.util.normalize(y)\n\n        expected_samples = sr * duration\n        if len(y) < expected_samples:\n            y = np.pad(y, (0, expected_samples - len(y)), mode=\"constant\")\n        else:\n            y = y[:expected_samples]\n\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n        mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n\n        plt.figure(figsize=(2, 2), dpi=64)\n        librosa.display.specshow(mel_db, sr=sr, cmap=\"magma\")\n        plt.axis(\"off\")\n        output_image_path.parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(output_image_path, bbox_inches=\"tight\", pad_inches=0)\n        plt.close()\n    except Exception as exc:\n        print(f\"[spectrogram] Failed on {audio_path}: {exc}\")\n\ndef _compute_progress(files, output_dir: Path):\n    total = len(files)\n    processed = sum((output_dir / f\"{Path(f).stem}.png\").exists() for f in files)\n    return total, processed\n\ndef process_audio_folder(input_dir: Path, output_dir: Path, desc: str):\n    if not input_dir.exists():\n        print(f\"[spectrogram] {input_dir} missing, skipping {desc}.\")\n        return\n    wav_files = sorted([f for f in input_dir.iterdir() if f.suffix.lower() == \".wav\"])\n    total, processed = _compute_progress([f.name for f in wav_files], output_dir)\n    with tqdm(total=total, initial=processed, desc=desc, unit=\"file\") as pbar:\n        for wav_path in wav_files:\n            out_path = output_dir / f\"{wav_path.stem}.png\"\n            if out_path.exists():\n                pbar.update(1)\n                continue\n            preprocess_and_save_spectrogram(wav_path, out_path)\n            gc.collect()\n            pbar.update(1)\n\ndef process_external_folder(input_dir: Path, output_dir: Path):\n    if not input_dir.exists():\n        print(\"[spectrogram] External noise folder missing, skipping.\")\n        return\n    audio_paths = []\n    for root, _, files in os.walk(input_dir):\n        audio_paths += [Path(root) / f for f in files if f.lower().endswith(\".wav\")]\n    with tqdm(total=len(audio_paths), desc=\"External noise\", unit=\"file\") as pbar:\n        for wav_path in audio_paths:\n            out_path = output_dir / f\"{wav_path.stem}.png\"\n            if out_path.exists():\n                pbar.update(1)\n                continue\n            preprocess_and_save_spectrogram(wav_path, out_path)\n            pbar.update(1)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:14:05.203225Z",
     "iopub.execute_input": "2025-12-23T09:14:05.203741Z",
     "iopub.status.idle": "2025-12-23T09:14:05.217251Z",
     "shell.execute_reply.started": "2025-12-23T09:14:05.203713Z",
     "shell.execute_reply": "2025-12-23T09:14:05.216498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "[Memory(location=None)]: Flushing completely the cache\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 22
  },
  {
   "id": "5194f6d3",
   "cell_type": "code",
   "source": "process_audio_folder(QUEEN_PRESENT_DIR, SPECTROGRAM_PRESENT, \"QueenBee Present\")\nprocess_audio_folder(QUEEN_ABSENT_DIR, SPECTROGRAM_ABSENT, \"QueenBee Absent\")\nprocess_external_folder(EXTERNAL_DIR, SPECTROGRAM_EXTERNAL)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:14:11.660861Z",
     "iopub.execute_input": "2025-12-23T09:14:11.661198Z",
     "iopub.status.idle": "2025-12-23T09:14:11.942176Z",
     "shell.execute_reply.started": "2025-12-23T09:14:11.661170Z",
     "shell.execute_reply": "2025-12-23T09:14:11.941593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "QueenBee Present: 8000file [00:00, 77675.89file/s]             \nQueenBee Absent: 4000file [00:00, 76544.68file/s]             \nExternal noise: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00<00:00, 60670.07file/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "id": "fb90b30b",
   "cell_type": "code",
   "source": "def count_pngs(folder: Path):\n    return len([f for f in folder.glob(\"*.png\")])\n\nclass_labels = [\"present\", \"absent\", \"external\"]\ncounts = [\n    count_pngs(SPECTROGRAM_PRESENT),\n    count_pngs(SPECTROGRAM_ABSENT),\n    count_pngs(SPECTROGRAM_EXTERNAL),\n]\n\nplt.figure(figsize=(6, 4))\nbars = plt.bar(class_labels, counts, color=[\"sienna\", \"peru\", \"gray\"], edgecolor=\"black\")\nplt.ylim(0, max(counts) * 1.1 if counts else 10)\nplt.title(\"Spectrogram Count per Class\")\nplt.ylabel(\"Images\")\nfor bar in bars:\n    y = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, y + max(1, y ** 0.5), int(y), ha=\"center\", va=\"bottom\")\nplt.show()\n\nprint(dict(zip(class_labels, counts)))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:14:24.202810Z",
     "iopub.execute_input": "2025-12-23T09:14:24.203401Z",
     "iopub.status.idle": "2025-12-23T09:14:24.530862Z",
     "shell.execute_reply.started": "2025-12-23T09:14:24.203363Z",
     "shell.execute_reply": "2025-12-23T09:14:24.529948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "{'present': 4000, 'absent': 2000, 'external': 2000}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "id": "b1438348",
   "cell_type": "code",
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "BASE_BATCH_SIZE = 32\n",
    "BATCH_SIZE = BASE_BATCH_SIZE  # Keep per-device batch size stable on Kaggle\n",
    "SEED = 42\n",
    "\n",
    "spectro_records = []\n",
    "for class_dir in sorted(SPECTROGRAM_DIR.iterdir()):\n",
    "    if class_dir.is_dir():\n",
    "        label = class_dir.name\n",
    "        for img_path in class_dir.glob(\"*.png\"):\n",
    "            spectro_records.append({\"filepath\": str(img_path), \"label\": label})\n",
    "\n",
    "if not spectro_records:\n",
    "    raise RuntimeError(\"No spectrograms were generated; run preprocessing above first.\")\n",
    "\n",
    "spectro_df = pd.DataFrame(spectro_records)\n",
    "CLASS_NAMES = sorted(spectro_df[\"label\"].unique())\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    spectro_df,\n",
    "    test_size=0.4,\n",
    "    stratify=spectro_df[\"label\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05\n",
    ")\n",
    "eval_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    classes=CLASS_NAMES,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = eval_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    classes=CLASS_NAMES,\n",
    "    shuffle=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "test_gen = eval_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    classes=CLASS_NAMES,\n",
    "    shuffle=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "raw_class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array(CLASS_NAMES),\n",
    "    y=train_df[\"label\"]\n",
    ")\n",
    "CLASS_WEIGHTS = {\n",
    "    train_gen.class_indices[label]: weight for label, weight in zip(CLASS_NAMES, raw_class_weights)\n",
    "}\n",
    "print(\"Class indices:\", train_gen.class_indices)\n",
    "print(\"Class weights:\", CLASS_WEIGHTS)\n",
    "\n",
    "ABSENT_CLASS_INDEX = train_gen.class_indices[\"absent\"]\n",
    "\n",
    "class SparseClassRecall(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_id, name=\"sparse_class_recall\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.class_id = class_id\n",
    "        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.false_negatives = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "        y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
    "        class_mask = tf.cast(tf.equal(y_true, self.class_id), self.dtype)\n",
    "        pred_mask = tf.cast(tf.equal(y_pred, self.class_id), self.dtype)\n",
    "        if sample_weight is None:\n",
    "            weights = tf.ones_like(class_mask)\n",
    "        else:\n",
    "            weights = tf.cast(tf.reshape(sample_weight, [-1]), self.dtype)\n",
    "            weights = tf.broadcast_to(weights, tf.shape(class_mask))\n",
    "        weighted_mask = class_mask * weights\n",
    "        tp = tf.reduce_sum(pred_mask * weighted_mask)\n",
    "        fn = tf.reduce_sum((1.0 - pred_mask) * weighted_mask)\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"class_id\": int(self.class_id)})\n",
    "        return config\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0.0)\n",
    "        self.false_negatives.assign(0.0)\n",
    "\n",
    "def make_absent_recall(name=\"recall_absent\"):\n",
    "    return SparseClassRecall(class_id=ABSENT_CLASS_INDEX, name=name)\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:14:46.558286Z",
     "iopub.execute_input": "2025-12-23T09:14:46.558862Z",
     "iopub.status.idle": "2025-12-23T09:14:46.754688Z",
     "shell.execute_reply.started": "2025-12-23T09:14:46.558825Z",
     "shell.execute_reply": "2025-12-23T09:14:46.753966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 5600 images belonging to 3 classes.\nFound 4000 images belonging to 3 classes.\nFound 4000 images belonging to 3 classes.\nClass indices: {'absent': 0, 'external': 1, 'present': 2}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 25
  },
  {
   "id": "a233e93a",
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_baseline_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", make_absent_recall()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "with strategy.scope():\n",
    "    baseline_model = build_baseline_model()\n",
    "\n",
    "baseline_callbacks = [\n",
    "    EarlyStopping(monitor=\"val_recall_absent\", mode=\"max\", patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "baseline_history = baseline_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    callbacks=baseline_callbacks\n",
    ")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T09:15:00.586135Z",
     "iopub.execute_input": "2025-12-23T09:15:00.586436Z",
     "iopub.status.idle": "2025-12-23T09:17:07.421212Z",
     "shell.execute_reply.started": "2025-12-23T09:15:00.586411Z",
     "shell.execute_reply": "2025-12-23T09:17:07.420646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nEpoch 1/10\nINFO:tensorflow:Collective all_reduce tensors: 16 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "I0000 00:00:1766481304.711346    4582 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1766481306.260956    4584 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7643 - loss: 0.5874INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 138ms/step - accuracy: 0.7654 - loss: 0.5851 - val_accuracy: 0.5000 - val_loss: 1.1700\nEpoch 2/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 132ms/step - accuracy: 0.9596 - loss: 0.1312 - val_accuracy: 0.5000 - val_loss: 3.2782\nEpoch 3/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 132ms/step - accuracy: 0.9805 - loss: 0.0736 - val_accuracy: 0.5000 - val_loss: 5.1786\nEpoch 4/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 137ms/step - accuracy: 0.9887 - loss: 0.0481 - val_accuracy: 0.5070 - val_loss: 6.9938\nEpoch 5/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 131ms/step - accuracy: 0.9806 - loss: 0.0503 - val_accuracy: 0.5573 - val_loss: 4.8722\nEpoch 6/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 131ms/step - accuracy: 0.9873 - loss: 0.0410 - val_accuracy: 0.6390 - val_loss: 2.2687\nEpoch 7/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 129ms/step - accuracy: 0.9890 - loss: 0.0357 - val_accuracy: 0.6308 - val_loss: 1.3170\nEpoch 8/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 131ms/step - accuracy: 0.9919 - loss: 0.0254 - val_accuracy: 0.8062 - val_loss: 0.6559\nEpoch 9/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 133ms/step - accuracy: 0.9947 - loss: 0.0175 - val_accuracy: 0.3738 - val_loss: 8.0576\nEpoch 10/10\n\u001b[1m88/88\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 132ms/step - accuracy: 0.9938 - loss: 0.0194 - val_accuracy: 0.8385 - val_loss: 0.3907\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 26
  },
  {
   "id": "75194ab4",
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pathlib import Path\n",
    "\n",
    "def build_tunable_model(hp):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(\n",
    "            hp.Choice(\"conv1\", [32, 64]), 3,\n",
    "            activation=\"relu\", padding=\"same\",\n",
    "            input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "        ),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.Conv2D(hp.Choice(\"conv2\", [64, 128]), 3, activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.Conv2D(hp.Choice(\"conv3\", [128, 256]), 3, activation=\"relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(hp.Int(\"dense_units\", 64, 128, step=32), activation=\"relu\"),\n",
    "        layers.Dropout(hp.Float(\"dropout\", 0.3, 0.6, step=0.1)),\n",
    "        layers.Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice(\"optimizer\", [\"adam\", \"nadam\"]),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", make_absent_recall()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Strategy ONLY for tuner creation\n",
    "with strategy.scope():\n",
    "    tuner = kt.Hyperband(\n",
    "        build_tunable_model,\n",
    "        objective=kt.Objective(\"val_recall_absent\", direction=\"max\"),\n",
    "        max_epochs=15,\n",
    "        factor=3,\n",
    "        directory=\"/kaggle/working/queenbee_tuning\",\n",
    "        project_name=\"queenbee_cnn\"\n",
    "    )\n",
    "\n",
    "stopper = EarlyStopping(\n",
    "    monitor=\"val_recall_absent\",\n",
    "    mode=\"max\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Search OUTSIDE strategy scope\n",
    "tuner.search(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    callbacks=[stopper]\n",
    ")\n",
    "\n",
    "# NO strategy scope here\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "fine_tune_history = best_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    callbacks=[stopper]\n",
    ")\n",
    "\n",
    "# Writable save path\n",
    "best_model_path = Path(\"/kaggle/working/queenbee_final_tuned_model.keras\")\n",
    "best_model.save(best_model_path)\n",
    "\n",
    "print(\"Saved tuned model to\", best_model_path)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T10:00:39.723452Z",
     "iopub.execute_input": "2025-12-23T10:00:39.723895Z",
     "iopub.status.idle": "2025-12-23T10:29:38.407668Z",
     "shell.execute_reply.started": "2025-12-23T10:00:39.723862Z",
     "shell.execute_reply": "2025-12-23T10:29:38.407038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Trial 30 Complete [00h 00m 53s]\nval_accuracy: 0.25\n\nBest val_accuracy So Far: 0.9837499856948853\nTotal elapsed time: 00h 27m 03s\nFound 6800 images belonging to 3 classes.\nFound 1200 images belonging to 3 classes.\nEpoch 1/10\n\u001b[1m 20/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u001b[0m\u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - accuracy: 0.9969 - loss: 0.0199",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2025-12-23 10:27:54.525831: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-23 10:27:54.694532: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 160ms/step - accuracy: 0.9959 - loss: 0.0160 - val_accuracy: 0.9325 - val_loss: 0.2259\nEpoch 2/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.9978 - loss: 0.0061 - val_accuracy: 0.6850 - val_loss: 3.3073\nEpoch 3/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - accuracy: 0.9979 - loss: 0.0068 - val_accuracy: 0.5808 - val_loss: 2.2780\nEpoch 4/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.9961 - loss: 0.0126 - val_accuracy: 0.2500 - val_loss: 14.4679\nEpoch 5/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.9968 - loss: 0.0084 - val_accuracy: 0.7100 - val_loss: 0.7533\nEpoch 6/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.7508 - val_loss: 1.3822\nEpoch 7/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.9951 - loss: 0.0143 - val_accuracy: 0.4517 - val_loss: 1.8356\nEpoch 8/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 0.4675 - val_loss: 4.3102\nEpoch 9/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.9974 - loss: 0.0082 - val_accuracy: 0.9467 - val_loss: 0.1476\nEpoch 10/10\n\u001b[1m107/107\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - accuracy: 0.9982 - loss: 0.0055 - val_accuracy: 0.6867 - val_loss: 3.3850\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Saved tuned model to /kaggle/working/queenbee_final_tuned_model.h5\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 31
  },
  {
   "id": "2e8f7167",
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_for_eval = load_model(\n",
    "    best_model_path,\n",
    "    custom_objects={\"SparseClassRecall\": SparseClassRecall}\n",
    ")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T10:30:17.748817Z",
     "iopub.execute_input": "2025-12-23T10:30:17.749469Z",
     "iopub.status.idle": "2025-12-23T10:30:17.887131Z",
     "shell.execute_reply.started": "2025-12-23T10:30:17.749441Z",
     "shell.execute_reply": "2025-12-23T10:30:17.886585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 32
  },
  {
   "id": "921011f7",
   "cell_type": "code",
   "source": [
    "def run_inference(model, generator):\n",
    "    generator.reset()\n",
    "    y_prob = model.predict(generator, verbose=1)\n",
    "    y_true = generator.classes\n",
    "    return y_prob, y_true\n",
    "\n",
    "\n",
    "def derive_thresholds(y_true, y_prob, class_names):\n",
    "    y_true_oh = tf.keras.utils.to_categorical(y_true, num_classes=len(class_names))\n",
    "    thresholds = {}\n",
    "    for idx, name in enumerate(class_names):\n",
    "        precision, recall, thresh = precision_recall_curve(y_true_oh[:, idx], y_prob[:, idx])\n",
    "        if thresh.size == 0:\n",
    "            thresholds[name] = 0.5\n",
    "            continue\n",
    "        f1 = 2 * precision * recall / np.clip(precision + recall, 1e-8, None)\n",
    "        best_idx = np.nanargmax(f1)\n",
    "        thresholds[name] = float(thresh[min(best_idx, len(thresh) - 1)])\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def predict_with_thresholds(y_prob, class_names, thresholds):\n",
    "    calibrated = []\n",
    "    for row in y_prob:\n",
    "        chosen_idx = None\n",
    "        chosen_score = -1.0\n",
    "        for idx, name in enumerate(class_names):\n",
    "            threshold = thresholds.get(name, 0.5)\n",
    "            if row[idx] >= threshold and row[idx] > chosen_score:\n",
    "                chosen_idx = idx\n",
    "                chosen_score = row[idx]\n",
    "        if chosen_idx is None:\n",
    "            chosen_idx = int(np.argmax(row))\n",
    "        calibrated.append(chosen_idx)\n",
    "    return np.array(calibrated)\n",
    "\n",
    "\n",
    "def summarize_metrics(y_true, y_pred, label):\n",
    "    return {\n",
    "        \"Mode\": label,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro Precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"Macro Recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"Macro F1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    }\n",
    "\n",
    "val_prob, val_true = run_inference(model_for_eval, val_gen)\n",
    "class_names = list(test_gen.class_indices.keys())\n",
    "thresholds = derive_thresholds(val_true, val_prob, class_names)\n",
    "print(\"Calibrated probability thresholds:\")\n",
    "for name in class_names:\n",
    "    print(f\"  {name}: {thresholds[name]:.3f}\")\n",
    "\n",
    "\n",
    "test_prob, test_true = run_inference(model_for_eval, test_gen)\n",
    "default_pred = np.argmax(test_prob, axis=1)\n",
    "calibrated_pred = predict_with_thresholds(test_prob, class_names, thresholds)\n",
    "\n",
    "metrics_table = pd.DataFrame([\n",
    "    summarize_metrics(test_true, default_pred, \"Argmax\"),\n",
    "    summarize_metrics(test_true, calibrated_pred, \"Calibrated\")\n",
    "])\n",
    "display(metrics_table)\n",
    "\n",
    "cm = confusion_matrix(test_true, calibrated_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Calibrated)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Calibrated classification report:\n",
    "\", classification_report(test_true, calibrated_pred, target_names=class_names, zero_division=0))\n",
    "\n",
    "roc_auc = roc_auc_score(\n",
    "    pd.get_dummies(test_true, drop_first=False).values,\n",
    "    test_prob,\n",
    "    average=\"macro\",\n",
    "    multi_class=\"ovr\"\n",
    ")\n",
    "pr_auc = average_precision_score(\n",
    "    pd.get_dummies(test_true, drop_first=False).values,\n",
    "    test_prob,\n",
    "    average=\"macro\"\n",
    ")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-23T10:30:23.496643Z",
     "iopub.execute_input": "2025-12-23T10:30:23.497345Z",
     "iopub.status.idle": "2025-12-23T10:30:28.750966Z",
     "shell.execute_reply.started": "2025-12-23T10:30:23.497319Z",
     "shell.execute_reply": "2025-12-23T10:30:28.750231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[1m63/63\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 67ms/step\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "            Metric     Score\n0         Accuracy  0.720500\n1  Macro Precision  0.880479\n2     Macro Recall  0.627333\n3         Macro F1  0.577196",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Accuracy</td>\n      <td>0.720500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Macro Precision</td>\n      <td>0.880479</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Macro Recall</td>\n      <td>0.627333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Macro F1</td>\n      <td>0.577196</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "              precision    recall  f1-score   support\n\n      absent       1.00      0.01      0.02      1000\n    external       1.00      0.87      0.93      1000\n     present       0.64      1.00      0.78      2000\n\n    accuracy                           0.72      4000\n   macro avg       0.88      0.63      0.58      4000\nweighted avg       0.82      0.72      0.63      4000\n\nROC-AUC: 0.9923 | PR-AUC: 0.9815\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 33
  },
  {
   "id": "8a188280",
   "cell_type": "code",
   "source": "SR = 22050\n\ndef audio_to_spectrogram_image(audio_path: Path):\n    y, sr = librosa.load(audio_path, sr=SR)\n    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n    S_dB = librosa.power_to_db(S, ref=np.max)\n\n    fig = plt.figure(figsize=(2, 2), dpi=64)\n    librosa.display.specshow(S_dB, sr=sr, cmap=\"magma\")\n    plt.axis(\"off\")\n\n    buf = io.BytesIO()\n    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close(fig)\n    buf.seek(0)\n\n    img = Image.open(buf).convert(\"RGB\").resize(IMG_SIZE)\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n    return img_array\n\ndef visualize_audio_prediction(audio_path: Path, model):\n    mel_input = audio_to_spectrogram_image(audio_path)\n    prediction = model.predict(mel_input)\n    class_names = list(test_gen.class_indices.keys())\n    pred_idx = int(np.argmax(prediction))\n    confidence = float(np.max(prediction))\n\n    y, sr = librosa.load(audio_path, sr=SR)\n    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n    mel_db = librosa.power_to_db(mel, ref=np.max)\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    times = np.linspace(0, len(y)/sr, len(y))\n    axes[0,0].plot(times, y)\n    axes[0,0].set_title(\"Waveform\")\n\n    img = axes[0,1].imshow(mel_db, aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n    axes[0,1].set_title(\"Mel Spectrogram\")\n    plt.colorbar(img, ax=axes[0,1], fraction=0.046, pad=0.04)\n\n    axes[1,0].bar(class_names, prediction[0], color=\"teal\")\n    axes[1,0].set_ylim(0, 1)\n    axes[1,0].set_title(\"Prediction Probabilities\")\n\n    axes[1,1].axis(\"off\")\n    axes[1,1].text(0.1, 0.5, f\"Predicted: {class_names[pred_idx]}\\nConfidence: {confidence:.2%}\", fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\"prediction\": class_names[pred_idx], \"confidence\": confidence}\n\n# sample_audio = next(QUEEN_PRESENT_DIR.glob('*.wav'))\n# visualize_audio_prediction(sample_audio, model_for_eval)\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "86e1ed93",
   "cell_type": "markdown",
   "source": "## Makueni Apiary Intelligence Pipeline",
   "metadata": {}
  },
  {
   "id": "6c6b0129",
   "cell_type": "code",
   "source": "DEFAULT_CENTER = (-1.8048, 37.62)\n\ntry:\n    import ipywidgets as widgets\n    from ipyleaflet import Map, Marker, DrawControl, basemaps\nexcept Exception:\n    print(\"ipyleaflet not available; using default coordinates.\")\n    lat_widget = lon_widget = geometry_widget = None\nelse:\n    lat_widget = widgets.FloatText(value=DEFAULT_CENTER[0], description=\"Latitude\", step=0.0001)\n    lon_widget = widgets.FloatText(value=DEFAULT_CENTER[1], description=\"Longitude\", step=0.0001)\n    geometry_widget = widgets.Textarea(\n        value=\"\",\n        description=\"Geometry\",\n        placeholder=\"Draw a polygon/rectangle on the map.\",\n        layout=widgets.Layout(width=\"100%\", height=\"140px\"),\n        disabled=True,\n    )\n\n    leaflet_map = Map(center=DEFAULT_CENTER, zoom=8, basemap=basemaps.OpenStreetMap.Mapnik, scroll_wheel_zoom=True)\n    marker = Marker(location=DEFAULT_CENTER, draggable=True)\n    leaflet_map.add_layer(marker)\n\n    draw_control = DrawControl(\n        polygon={\"shapeOptions\": {\"color\": \"#2563eb\", \"weight\": 2, \"fillOpacity\": 0.2}},\n        rectangle={\"shapeOptions\": {\"color\": \"#f97316\", \"weight\": 2, \"fillOpacity\": 0.15}},\n        circle={},\n        circlemarker={},\n        polyline={},\n    )\n    leaflet_map.add_control(draw_control)\n\n    def _update_marker(change):\n        marker.location = (lat_widget.value, lon_widget.value)\n\n    lat_widget.observe(_update_marker, names=\"value\")\n    lon_widget.observe(_update_marker, names=\"value\")\n\n    display(widgets.HBox([lat_widget, lon_widget]))\n    display(geometry_widget)\n    display(leaflet_map)\n\nlat_widget_available = 'lat_widget' in globals() and lat_widget is not None\nlon_widget_available = 'lon_widget' in globals() and lon_widget is not None\n\nif lat_widget_available and lon_widget_available:\n    latitude = float(lat_widget.value)\n    longitude = float(lon_widget.value)\nelse:\n    latitude, longitude = DEFAULT_CENTER\n    print(\"Using default coordinates:\", DEFAULT_CENTER)\n\nselected_geometry_geojson = globals().get('selected_geometry_geojson')\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "13f4ff43",
   "cell_type": "code",
   "source": "raw_start_date = \"2008-01-01\"\nraw_end_date = \"2025-12-05\"\ntimezone = \"Africa/Nairobi\"\n\ndef normalize_date_string(d: str) -> dt.date:\n    parts = d.split(\"-\")\n    if len(parts) != 3:\n        raise ValueError(\"Date must be YYYY-MM-DD\")\n    y, m, day = [int(p) for p in parts]\n    m = max(1, min(12, m))\n    last_day = calendar.monthrange(y, m)[1]\n    day = max(1, min(last_day, day))\n    return dt.date(y, m, day)\n\nstart_date = normalize_date_string(raw_start_date)\nend_date = normalize_date_string(raw_end_date)\n\ntoday = dt.date.today()\napi_latest = dt.date(2025, 12, 20)\nmax_allowed = min(today, api_latest)\n\nif end_date > max_allowed:\n    print(f\"Clamping end_date {end_date} -> {max_allowed}\")\n    end_date = max_allowed\nif start_date > end_date:\n    raise ValueError(\"start_date must be before end_date\")\n\nprint(\"Using date range:\", start_date, \"\u2192\", end_date)\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "f651dd70",
   "cell_type": "code",
   "source": "ENABLE_REMOTE_CALLS = False  # Kaggle notebooks typically block outbound internet.\n\ndef split_date_range(start: dt.date, end: dt.date, max_days: int = 365):\n    chunks = []\n    current = start\n    while current <= end:\n        chunk_end = min(end, current + dt.timedelta(days=max_days - 1))\n        chunks.append((current, chunk_end))\n        current = chunk_end + dt.timedelta(days=1)\n    return chunks\n\ndef fetch_chunk(lat, lon, sdate: dt.date, edate: dt.date, timezone=\"Africa/Nairobi\", max_retries=3, backoff=2):\n    base = \"https://archive-api.open-meteo.com/v1/archive\"\n    daily_vars = \",\".join([\n        \"temperature_2m_max\",\n        \"temperature_2m_min\",\n        \"temperature_2m_mean\",\n        \"precipitation_sum\",\n        \"relative_humidity_2m_mean\",\n        \"wind_speed_10m_max\",\n        \"cloudcover_mean\"\n    ])\n    params = {\n        \"latitude\": lat,\n        \"longitude\": lon,\n        \"start_date\": sdate.strftime(\"%Y-%m-%d\"),\n        \"end_date\": edate.strftime(\"%Y-%m-%d\"),\n        \"daily\": daily_vars,\n        \"timezone\": timezone\n    }\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = requests.get(base, params=params, timeout=30)\n            resp.raise_for_status()\n            payload = resp.json()\n            if \"daily\" not in payload or \"time\" not in payload[\"daily\"]:\n                raise ValueError(\"API response missing expected fields.\")\n            return payload\n        except Exception as exc:\n            print(f\"Attempt {attempt} failed: {exc}\")\n            if attempt == max_retries:\n                raise\n            time.sleep(backoff ** attempt)\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "b8b45a4d",
   "cell_type": "code",
   "source": "weather_csv = MAIN_DATA_DIR / \"makueni_weather_2008_2025.csv\"\nchunks = split_date_range(start_date, end_date, max_days=365)\n\nif ENABLE_REMOTE_CALLS:\n    dfs = []\n    for s, e in chunks:\n        payload = fetch_chunk(latitude, longitude, s, e, timezone=timezone)\n        daily = payload[\"daily\"]\n        df_chunk = pd.DataFrame({\n            \"date\": daily[\"time\"],\n            \"temp_max\": daily.get(\"temperature_2m_max\"),\n            \"temp_min\": daily.get(\"temperature_2m_min\"),\n            \"temp_mean\": daily.get(\"temperature_2m_mean\"),\n            \"humidity_mean\": daily.get(\"relative_humidity_2m_mean\"),\n            \"rainfall_mm\": daily.get(\"precipitation_sum\"),\n            \"wind_speed_max\": daily.get(\"wind_speed_10m_max\"),\n            \"cloud_cover_percent\": daily.get(\"cloudcover_mean\"),\n        })\n        dfs.append(df_chunk)\n        time.sleep(1)\n    weather_df = pd.concat(dfs, ignore_index=True)\n    weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n    weather_df.sort_values(\"date\", inplace=True)\n    weather_df.to_csv(weather_csv, index=False)\n    print(\"Fetched and saved weather CSV to\", weather_csv)\nelse:\n    if weather_csv.exists():\n        weather_df = pd.read_csv(weather_csv, parse_dates=[\"date\"])\n        print(f\"Loaded cached weather data from {weather_csv}\")\n    else:\n        raise FileNotFoundError(f\"{weather_csv} not found; enable ENABLE_REMOTE_CALLS to regenerate.\")\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "0bd89f0c",
   "cell_type": "code",
   "source": "ndvi_csv = MAIN_DATA_DIR / \"makueni_ndvi_2008_2025.csv\"\n\nif ENABLE_REMOTE_CALLS:\n    try:\n        ee.Initialize()\n    except Exception:\n        print(\"Authenticating with Earth Engine...\")\n        ee.Authenticate()\n        ee.Initialize()\n\n    point = ee.Geometry.Point([longitude, latitude])\n    modis = ee.ImageCollection(\"MODIS/061/MOD13Q1\").select(\"NDVI\").filterDate(start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")).filterBounds(point)\n\n    def extract_ndvi(image):\n        mean = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=point, scale=250).get(\"NDVI\")\n        date = image.date().format(\"YYYY-MM-dd\")\n        return ee.Feature(None, {\"date\": date, \"ndvi_mean\": mean})\n\n    ndvi_fc = modis.map(extract_ndvi).getInfo()\n    records = [f[\"properties\"] for f in ndvi_fc[\"features\"]]\n    ndvi_df = pd.DataFrame(records)\n    ndvi_df[\"date\"] = pd.to_datetime(ndvi_df[\"date\"])\n    ndvi_df[\"ndvi_mean\"] = ndvi_df[\"ndvi_mean\"].astype(float) / 10000\n    ndvi_df.to_csv(ndvi_csv, index=False)\n    print(\"Fetched NDVI and saved to\", ndvi_csv)\nelse:\n    if ndvi_csv.exists():\n        ndvi_df = pd.read_csv(ndvi_csv, parse_dates=[\"date\"])\n        print(f\"Loaded cached NDVI data from {ndvi_csv}\")\n    else:\n        raise FileNotFoundError(f\"{ndvi_csv} not found; enable ENABLE_REMOTE_CALLS to regenerate.\")\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "f84d3908",
   "cell_type": "code",
   "source": "df_weather = weather_df.copy()\ndf_ndvi = ndvi_df.copy()\n\ndf_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\ndf_ndvi[\"date\"] = pd.to_datetime(df_ndvi[\"date\"])\n\ndf_merged = pd.merge(df_weather, df_ndvi, on=\"date\", how=\"left\").sort_values(\"date\")\nweather_ndvi_path = MAIN_DATA_DIR / \"makueni_weather_ndvi_2008_2025.csv\"\ndf_merged.to_csv(weather_ndvi_path, index=False)\nprint(\"Merged weather+NDVI ->\", weather_ndvi_path)\ndf_merged.head()\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "27121437",
   "cell_type": "code",
   "source": "df_month = df_merged.set_index(\"date\").resample(\"ME\").agg({\n    \"rainfall_mm\": \"sum\",\n    \"temp_mean\": \"mean\",\n    \"humidity_mean\": \"mean\",\n    \"ndvi_mean\": \"mean\"\n}).reset_index()\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\naxes[0].plot(df_month[\"date\"], df_month[\"rainfall_mm\"], marker=\"o\")\naxes[0].set_title(\"Monthly Rainfall (mm)\")\n\naxes[1].plot(df_month[\"date\"], df_month[\"temp_mean\"], marker=\"o\", color=\"tomato\")\naxes[1].set_title(\"Monthly Mean Temperature (\u00b0C)\")\n\naxes[2].plot(df_month[\"date\"], df_month[\"ndvi_mean\"], marker=\"o\", color=\"green\")\naxes[2].set_title(\"Monthly NDVI Mean\")\n\nfor ax in axes:\n    ax.grid(True, alpha=0.3)\n    ax.set_ylabel(\"Value\")\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "8dea5e59",
   "cell_type": "code",
   "source": "hive_logs_path = MAIN_DATA_DIR / \"hive_logs_2008_2025.csv\"\n\nif hive_logs_path.exists():\n    hive_df = pd.read_csv(hive_logs_path, parse_dates=[\"date\"])\n    print(\"Loaded hive logs from\", hive_logs_path)\nelse:\n    print(\"Generating synthetic hive telemetry...\")\n    start_dt = dt.datetime(2008, 1, 1)\n    end_dt = dt.datetime(2025, 9, 30)\n    dates = pd.date_range(start=start_dt, end=end_dt, freq=\"7D\")\n\n    hive_ids = [\"Hive-A\", \"Hive-B\", \"Hive-C\", \"Hive-D\"]\n    data = []\n    rng = np.random.default_rng(42)\n    for hive in hive_ids:\n        queen_age = rng.integers(3, 20)\n        for date in dates:\n            honey_yield = max(0, rng.normal(12, 3))\n            varroa = np.clip(rng.normal(8, 3), 0, 40)\n            hive_weight = rng.normal(45, 5)\n            brood_area = np.clip(rng.normal(800, 150), 100, 1200)\n            stress_event = rng.choice([\"none\", \"ants\", \"drought\"], p=[0.85, 0.1, 0.05])\n            data.append({\n                \"date\": date,\n                \"hive_id\": hive,\n                \"honey_yield_kg\": honey_yield,\n                \"varroa_pct\": varroa,\n                \"hive_weight_kg\": hive_weight,\n                \"brood_area_cm2\": brood_area,\n                \"stress_event\": stress_event,\n                \"queen_age_months\": queen_age\n            })\n    hive_df = pd.DataFrame(data)\n    hive_df.to_csv(hive_logs_path, index=False)\n    print(\"Synthetic hive logs saved to\", hive_logs_path)\n\nhive_df.head()\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "3c6f516c",
   "cell_type": "code",
   "source": "weather_full = pd.read_csv(weather_ndvi_path, parse_dates=[\"date\"])\nhive_df[\"date\"] = pd.to_datetime(hive_df[\"date\"])\nmerged = pd.merge(hive_df, weather_full, on=\"date\", how=\"left\")\nmerged_path = MAIN_DATA_DIR / \"merged_hive_weather_ndvi.csv\"\nmerged.to_csv(merged_path, index=False)\nprint(\"Merged hive + weather ->\", merged_path)\nmerged.head()\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "82d3330c",
   "cell_type": "code",
   "source": "floral_data = {\n    \"date\": [\n        \"2025-01-15\",\"2025-02-15\",\"2025-03-15\",\"2025-04-15\",\n        \"2025-05-15\",\"2025-06-15\",\"2025-07-15\",\"2025-08-15\",\"2025-09-15\"\n    ],\n    \"major_flowers\": [\n        \"Acacia tortilis, Mango, Commiphora\",\n        \"Acacia tortilis, Acacia mellifera, Mango\",\n        \"Croton, Acacia mellifera\",\n        \"Croton, Melia volkensii\",\n        \"Citrus, Croton\",\n        \"Aloe, Citrus\",\n        \"Aloe, Pasture weeds\",\n        \"Eucalyptus, Pasture weeds\",\n        \"Eucalyptus camaldulensis\"\n    ],\n    \"nectar_flow_strength\": [\"High\",\"High\",\"Medium\",\"Medium\",\"Medium\",\"Medium\",\"Low\",\"Low-Medium\",\"High\"],\n    \"stress_risk\": [\"Low\",\"Low\",\"Medium\",\"Low\",\"Medium\",\"Medium\",\"High\",\"High\",\"Low\"],\n    \"pest_disease_notes\": [\n        \"Hive beetles active\",\n        \"Wax moth pressure\",\n        \"Varroa buildup\",\n        \"Chalkbrood risk\",\n        \"Nosema risk\",\n        \"Slow brood buildup\",\n        \"Ant invasions\",\n        \"Weak colony pests\",\n        \"Healthy buildup\"\n    ]\n}\nfloral_df = pd.DataFrame(floral_data)\nfloral_df[\"date\"] = pd.to_datetime(floral_df[\"date\"])\nfloral_path = MAIN_DATA_DIR / \"makueni_floral_calendar_2025.csv\"\nfloral_df.to_csv(floral_path, index=False)\nprint(\"Floral calendar saved to\", floral_path)\nfloral_df\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "5f5ec06d",
   "cell_type": "code",
   "source": "merged_full = merged.merge(floral_df, on=\"date\", how=\"left\")\nmerged_full_path = MAIN_DATA_DIR / \"merged_hive_weather_floral_2025.csv\"\nmerged_full.to_csv(merged_full_path, index=False)\nprint(\"Merged hive/weather/floral ->\", merged_full_path)\nmerged_full.head()\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "43772c05",
   "cell_type": "code",
   "source": "model_df = merged_full.copy()\nmodel_df[\"stress_event\"] = model_df[\"stress_event\"].fillna(\"none\")\nmodel_df[\"stress_target\"] = (model_df[\"stress_event\"] != \"none\").astype(int)\nmodel_df[\"date\"] = pd.to_datetime(model_df[\"date\"])\nmodel_df[\"month\"] = model_df[\"date\"].dt.month\nmodel_df[\"year\"] = model_df[\"date\"].dt.year\nmodel_df[\"weekofyear\"] = model_df[\"date\"].dt.isocalendar().week.astype(int)\n\nrolling_features = [\"honey_yield_kg\", \"varroa_pct\", \"hive_weight_kg\", \"brood_area_cm2\"]\nfor feature in rolling_features:\n    if feature in model_df.columns:\n        model_df[f\"{feature}_rolling_mean\"] = (\n            model_df.groupby(\"hive_id\")[feature]\n            .transform(lambda s: s.rolling(window=4, min_periods=1).mean())\n        )\n        model_df[f\"{feature}_rolling_std\"] = (\n            model_df.groupby(\"hive_id\")[feature]\n            .transform(lambda s: s.rolling(window=4, min_periods=1).std())\n        )\n\nnumeric_cols = model_df.select_dtypes(include=[np.number]).columns\nexclude_cols = {\"stress_target\"}\nfeature_cols = [col for col in numeric_cols if col not in exclude_cols]\nX = model_df[feature_cols].copy()\nX = X.dropna(axis=1, how=\"all\")\nfeature_cols = list(X.columns)\n\nimputer = SimpleImputer(strategy=\"median\")\nX = pd.DataFrame(imputer.fit_transform(X), columns=feature_cols, index=model_df.index)\ny = model_df[\"stress_target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\nclass_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nhb_model = HistGradientBoostingClassifier(\n    max_depth=6,\n    learning_rate=0.08,\n    max_iter=400,\n    class_weight=class_weight_dict\n)\n\nhb_model.fit(X_train, y_train)\ny_pred = hb_model.predict(X_test)\ny_prob = hb_model.predict_proba(X_test)[:, 1]\n\nprint(classification_report(y_test, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n\nRocCurveDisplay.from_predictions(y_test, y_prob)\nplt.show()\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "38cc1701",
   "cell_type": "code",
   "source": "WINDOW_SIZE = 12\nfeature_columns = [col for col in feature_cols if col in model_df.columns]\nsequence_features = model_df[feature_columns].fillna(model_df[feature_columns].median()).copy()\nsequence_targets = model_df['stress_target'].values\n\nX_sequences = []\ny_sequences = []\nmetadata = []\nfor hive_id, group in model_df.groupby('hive_id'):\n    group = group.sort_values('date')\n    features = group[feature_columns].fillna(group[feature_columns].median()).values\n    targets = group['stress_target'].values\n    dates = group['date'].values\n    if len(group) <= WINDOW_SIZE:\n        continue\n    for idx in range(WINDOW_SIZE, len(group)):\n        window = features[idx-WINDOW_SIZE:idx]\n        X_sequences.append(window)\n        y_sequences.append(targets[idx])\n        metadata.append({\"hive_id\": hive_id, \"date\": dates[idx]})\n\nX_sequences = np.array(X_sequences, dtype=np.float32)\ny_sequences = np.array(y_sequences, dtype=np.float32)\nprint(f'Total sequences: {X_sequences.shape[0]} | window shape: {X_sequences.shape[1:]}')\n\nclass SequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        self.X = torch.tensor(sequences, dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nX_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n    X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences\n)\n\ntrain_dataset = SequenceDataset(X_train_seq, y_train_seq)\nval_dataset = SequenceDataset(X_test_seq, y_test_seq)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n\ninput_channels = X_sequences.shape[-1]\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass HiveCNN(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv1d(channels, 64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        x = x.transpose(1, 2)\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.squeeze(-1)\n\nmodel = HiveCNN(input_channels).to(device)\npos_weight_value = float(max(1.0, (len(y_train_seq) - y_train_seq.sum()) / max(1.0, y_train_seq.sum())))\ncriterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_value, device=device))\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\ndef run_epoch(loader, train=True):\n    model.train(train)\n    total_loss = 0\n    preds, targets = [], []\n    for batch_X, batch_y in loader:\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(train):\n            logits = model(batch_X)\n            loss = criterion(logits, batch_y)\n            if train:\n                loss.backward()\n                optimizer.step()\n        total_loss += loss.item() * batch_X.size(0)\n        preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n        targets.append(batch_y.detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    targets = np.concatenate(targets)\n    return total_loss / len(loader.dataset), roc_auc_score(targets, preds)\n\nepochs = 50\nbest_auc = 0\npatience = 8\npatience_counter = 0\nmodel_path = MAIN_DATA_DIR / \"hive_cnn_torch.pt\"\n\nfor epoch in range(epochs):\n    train_loss, train_auc = run_epoch(train_loader, train=True)\n    val_loss, val_auc = run_epoch(val_loader, train=False)\n    print(f\"Epoch {epoch+1:02d}: train_loss={train_loss:.4f} AUC={train_auc:.3f} | val_loss={val_loss:.4f} AUC={val_auc:.3f}\")\n    if val_auc > best_auc + 1e-3:\n        best_auc = val_auc\n        patience_counter = 0\n        torch.save(model.state_dict(), model_path)\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\nwith torch.no_grad():\n    logits = []\n    labels = []\n    for batch_X, batch_y in val_loader:\n        batch_X = batch_X.to(device)\n        logits.append(model(batch_X).cpu())\n        labels.append(batch_y)\n    logits = torch.cat(logits)\n    labels = torch.cat(labels)\n    probs = torch.sigmoid(logits).numpy()\n    labels_np = labels.numpy()\n\nprecision, recall, thresholds = precision_recall_curve(labels_np, probs)\nf_scores = (2 * precision * recall) / np.clip(precision + recall, 1e-8, None)\nbest_idx = np.argmax(f_scores)\nbest_threshold = thresholds[max(best_idx - 1, 0)] if best_idx < len(thresholds) else 0.5\npreds = (probs >= best_threshold).astype(int)\n\nprint(f\"Best threshold based on F1: {best_threshold:.3f}\")\nprint(classification_report(labels_np, preds))\nprint('Test ROC-AUC:', roc_auc_score(labels_np, probs))\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\naxes[0].plot(recall, precision)\naxes[0].set_title('Precision-Recall')\naxes[0].set_xlabel('Recall')\naxes[0].set_ylabel('Precision')\nRocCurveDisplay.from_predictions(labels_np, probs, ax=axes[1])\naxes[1].set_title('ROC Curve')\nplt.tight_layout()\nplt.show()\n\nprint('Best model weights saved to', model_path)\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}