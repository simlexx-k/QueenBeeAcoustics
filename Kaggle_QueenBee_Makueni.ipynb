{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 12134294,
     "sourceType": "datasetVersion",
     "datasetId": 7505074
    },
    {
     "sourceId": 14288414,
     "sourceType": "datasetVersion",
     "datasetId": 9120225
    }
   ],
   "dockerImageVersionId": 31234,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "fdc48dcf",
   "cell_type": "markdown",
   "source": "# Kaggle Cloud Ops: Queen Bee Acoustics + Makueni Apiary Intelligence",
   "metadata": {}
  },
  {
   "id": "9325b117",
   "cell_type": "markdown",
   "source": "This unified notebook stitches together:\n\n1. **Queen Bee acoustic detection (CNN + hyperparameter tuning)**\n2. **Makueni Apiary intelligence workflows (weather, NDVI, telemetry, hive stress ML)**\n\n> **Kaggle usage:** Attach the `harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen` dataset plus any `content/main-data` exports as Kaggle data sources. All intermediate files are written under `content/` so the same notebook also works locally.",
   "metadata": {}
  },
  {
   "id": "ffc483a3",
   "cell_type": "code",
   "source": "!pip install -q earthengine-api ipyleaflet ipywidgets keras-tuner librosa tqdm",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:18:46.028553Z",
     "iopub.execute_input": "2025-12-25T06:18:46.029269Z",
     "iopub.status.idle": "2025-12-25T06:18:51.644794Z",
     "shell.execute_reply.started": "2025-12-25T06:18:46.029241Z",
     "shell.execute_reply": "2025-12-25T06:18:51.643892Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "id": "422e9772",
   "cell_type": "code",
   "source": "import os\nimport shutil\nfrom pathlib import Path\n\nPROJECT_ROOT = Path.cwd()\nDEFAULT_CONTENT = PROJECT_ROOT / \"content\"\nKAGGLE_WORKING = Path(\"/kaggle/working\")\n\nif DEFAULT_CONTENT.exists():\n    CONTENT_ROOT = DEFAULT_CONTENT.resolve()\nelse:\n    CONTENT_ROOT = (KAGGLE_WORKING / \"content\").resolve()\n    CONTENT_ROOT.mkdir(parents=True, exist_ok=True)\n\nos.environ[\"MERGED_CONTENT_ROOT\"] = str(CONTENT_ROOT)\nMAIN_DATA_DIR = (CONTENT_ROOT / \"main-data\")\nMAIN_DATA_DIR.mkdir(parents=True, exist_ok=True)\n\nKAGGLE_INPUT_ROOT = Path(\"/kaggle/input\")\n\ndef _stage_dataset(keyword, target_subdir):\n    if not KAGGLE_INPUT_ROOT.exists():\n        return None\n    matches = [p for p in KAGGLE_INPUT_ROOT.iterdir() if keyword in p.name.lower()]\n    if not matches:\n        print(f\"[setup] Kaggle input dataset containing '{keyword}' not found.\")\n        return None\n    source = matches[0]\n    target = CONTENT_ROOT / target_subdir\n    shutil.rmtree(target, ignore_errors=True)\n    shutil.copytree(source, target, dirs_exist_ok=True)\n    print(f\"[setup] Staged {source.name} -> {target}\")\n    return target\n\ndef _maybe_stage(keyword, subdir):\n    try:\n        _stage_dataset(keyword, subdir)\n    except Exception as exc:\n        print(f\"[setup] Skipping auto-stage for {keyword}: {exc}\")\n\n_maybe_stage(\"beehive\", \"beehive_audio\")\n_maybe_stage(\"makueni\", \"main-data\")\n\nprint(f\"CONTENT_ROOT -> {CONTENT_ROOT}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:18:51.646451Z",
     "iopub.execute_input": "2025-12-25T06:18:51.646815Z",
     "iopub.status.idle": "2025-12-25T06:23:16.431291Z",
     "shell.execute_reply.started": "2025-12-25T06:18:51.646748Z",
     "shell.execute_reply": "2025-12-25T06:23:16.430622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "[setup] Staged beehive-audio-dataset-with-queen-and-without-queen -> /kaggle/working/content/beehive_audio\n[setup] Kaggle input dataset containing 'makueni' not found.\nCONTENT_ROOT -> /kaggle/working/content\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "id": "e8446e4b",
   "cell_type": "code",
   "source": [
    "import calendar\n",
    "import datetime as dt\n",
    "import gc\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras_tuner as kt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import requests\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "CONTENT_ROOT = Path(os.environ[\"MERGED_CONTENT_ROOT\"])\n",
    "MAIN_DATA_DIR = CONTENT_ROOT / \"main-data\"\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:16.432117Z",
     "iopub.execute_input": "2025-12-25T06:23:16.432370Z",
     "iopub.status.idle": "2025-12-25T06:23:16.479499Z",
     "shell.execute_reply.started": "2025-12-25T06:23:16.432337Z",
     "shell.execute_reply": "2025-12-25T06:23:16.478826Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "id": "00311da8",
   "cell_type": "markdown",
   "source": "## Queen Bee Acoustic Detection Pipeline",
   "metadata": {}
  },
  {
   "id": "1c46188a",
   "cell_type": "code",
   "source": "from pathlib import Path\n\ndef _discover_audio_dataset(content_root: Path) -> Path:\n    search_root = Path(\"/kaggle/input/beehive-audio-dataset-with-queen-and-without-queen\")\n    if not search_root.exists():\n        raise FileNotFoundError(\n            \"Dataset not staged. Attach Kaggle dataset \"\n            \"'harshkumar1711/beehive-audio-dataset-with-queen-and-without-queen'.\"\n        )\n\n    for candidate in sorted(search_root.rglob(\"Dataset\")):\n        if (candidate / \"Bee Hive Audios\").exists():\n            return candidate\n\n    raise FileNotFoundError(\"Could not locate 'Dataset/Bee Hive Audios'.\")\n\n# Discover dataset (READ-ONLY)\nAUDIO_DATASET_ROOT = _discover_audio_dataset(None)\n\nBEEHIVE_AUDIO_DIR = next(AUDIO_DATASET_ROOT.glob(\"**/Bee Hive Audios\"))\nQUEEN_PRESENT_DIR = BEEHIVE_AUDIO_DIR / \"QueenBee Present\"\nQUEEN_ABSENT_DIR = BEEHIVE_AUDIO_DIR / \"QueenBee Absent\"\nEXTERNAL_DIR = AUDIO_DATASET_ROOT / \"External Noise\"\n\n# WRITEABLE spectrogram directory\nSPECTROGRAM_DIR = Path(\"/kaggle/working/spectrograms\")\nSPECTROGRAM_PRESENT = SPECTROGRAM_DIR / \"present\"\nSPECTROGRAM_ABSENT = SPECTROGRAM_DIR / \"absent\"\nSPECTROGRAM_EXTERNAL = SPECTROGRAM_DIR / \"external\"\n\nfor path in [SPECTROGRAM_PRESENT, SPECTROGRAM_ABSENT, SPECTROGRAM_EXTERNAL]:\n    path.mkdir(parents=True, exist_ok=True)\n\nprint(\"Audio dataset root (read-only):\", AUDIO_DATASET_ROOT)\nprint(\"Spectrogram cache (writable):\", SPECTROGRAM_DIR)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:16.481161Z",
     "iopub.execute_input": "2025-12-25T06:23:16.481412Z",
     "iopub.status.idle": "2025-12-25T06:23:26.089431Z",
     "shell.execute_reply.started": "2025-12-25T06:23:16.481394Z",
     "shell.execute_reply": "2025-12-25T06:23:26.088644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Audio dataset root (read-only): /kaggle/input/beehive-audio-dataset-with-queen-and-without-queen/Dataset\nSpectrogram cache (writable): /kaggle/working/spectrograms\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "id": "5601ca8a",
   "cell_type": "code",
   "source": "try:\n    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu_resolver)\n    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n    ACCELERATOR = \"TPU\"\nexcept (ValueError, tf.errors.NotFoundError):\n    gpus = tf.config.list_physical_devices(\"GPU\")\n    if gpus:\n        for gpu in gpus:\n            try:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            except Exception:\n                pass\n        # Default to single-replica strategy for Kaggle GPU stability\n        strategy = tf.distribute.get_strategy()\n        ACCELERATOR = \"GPU\"\n    else:\n        strategy = tf.distribute.get_strategy()\n        ACCELERATOR = \"CPU\"\n\nprint(f\"Using {ACCELERATOR} via {strategy.__class__.__name__}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:26.090455Z",
     "iopub.execute_input": "2025-12-25T06:23:26.090818Z",
     "iopub.status.idle": "2025-12-25T06:23:26.239636Z",
     "shell.execute_reply.started": "2025-12-25T06:23:26.090770Z",
     "shell.execute_reply": "2025-12-25T06:23:26.238957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Using GPU via _DefaultDistributionStrategy\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "id": "20b2348e",
   "cell_type": "code",
   "source": "SAMPLE_RATE = 22050\nDURATION = 3\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n\nlibrosa.cache.clear()\nplt.switch_backend(\"Agg\")\n\ndef preprocess_and_save_spectrogram(audio_path: Path, output_image_path: Path, sr=SAMPLE_RATE, duration=DURATION):\n    try:\n        y, _ = librosa.load(audio_path, sr=sr)\n        y, _ = librosa.effects.trim(y)\n        y = librosa.to_mono(y) if y.ndim > 1 else y\n        y = librosa.util.normalize(y)\n\n        expected_samples = sr * duration\n        if len(y) < expected_samples:\n            y = np.pad(y, (0, expected_samples - len(y)), mode=\"constant\")\n        else:\n            y = y[:expected_samples]\n\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n        mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n\n        plt.figure(figsize=(2, 2), dpi=64)\n        librosa.display.specshow(mel_db, sr=sr, cmap=\"magma\")\n        plt.axis(\"off\")\n        output_image_path.parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(output_image_path, bbox_inches=\"tight\", pad_inches=0)\n        plt.close()\n    except Exception as exc:\n        print(f\"[spectrogram] Failed on {audio_path}: {exc}\")\n\ndef _compute_progress(files, output_dir: Path):\n    total = len(files)\n    processed = sum((output_dir / f\"{Path(f).stem}.png\").exists() for f in files)\n    return total, processed\n\ndef process_audio_folder(input_dir: Path, output_dir: Path, desc: str):\n    if not input_dir.exists():\n        print(f\"[spectrogram] {input_dir} missing, skipping {desc}.\")\n        return\n    wav_files = sorted([f for f in input_dir.iterdir() if f.suffix.lower() == \".wav\"])\n    total, processed = _compute_progress([f.name for f in wav_files], output_dir)\n    with tqdm(total=total, initial=processed, desc=desc, unit=\"file\") as pbar:\n        for wav_path in wav_files:\n            out_path = output_dir / f\"{wav_path.stem}.png\"\n            if out_path.exists():\n                pbar.update(1)\n                continue\n            preprocess_and_save_spectrogram(wav_path, out_path)\n            gc.collect()\n            pbar.update(1)\n\ndef process_external_folder(input_dir: Path, output_dir: Path):\n    if not input_dir.exists():\n        print(\"[spectrogram] External noise folder missing, skipping.\")\n        return\n    audio_paths = []\n    for root, _, files in os.walk(input_dir):\n        audio_paths += [Path(root) / f for f in files if f.lower().endswith(\".wav\")]\n    with tqdm(total=len(audio_paths), desc=\"External noise\", unit=\"file\") as pbar:\n        for wav_path in audio_paths:\n            out_path = output_dir / f\"{wav_path.stem}.png\"\n            if out_path.exists():\n                pbar.update(1)\n                continue\n            preprocess_and_save_spectrogram(wav_path, out_path)\n            pbar.update(1)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:26.240906Z",
     "iopub.execute_input": "2025-12-25T06:23:26.241392Z",
     "iopub.status.idle": "2025-12-25T06:23:26.434415Z",
     "shell.execute_reply.started": "2025-12-25T06:23:26.241366Z",
     "shell.execute_reply": "2025-12-25T06:23:26.433896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "[Memory(location=None)]: Flushing completely the cache\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "id": "5194f6d3",
   "cell_type": "code",
   "source": "process_audio_folder(QUEEN_PRESENT_DIR, SPECTROGRAM_PRESENT, \"QueenBee Present\")\nprocess_audio_folder(QUEEN_ABSENT_DIR, SPECTROGRAM_ABSENT, \"QueenBee Absent\")\nprocess_external_folder(EXTERNAL_DIR, SPECTROGRAM_EXTERNAL)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:26.435196Z",
     "iopub.execute_input": "2025-12-25T06:23:26.435405Z",
     "iopub.status.idle": "2025-12-25T06:23:28.077224Z",
     "shell.execute_reply.started": "2025-12-25T06:23:26.435388Z",
     "shell.execute_reply": "2025-12-25T06:23:28.076645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "QueenBee Present: 8000file [00:00, 80026.41file/s]             \nQueenBee Absent: 4000file [00:00, 64880.10file/s]             \nExternal noise: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00<00:00, 60924.76file/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "fb90b30b",
   "cell_type": "code",
   "source": "def count_pngs(folder: Path):\n    return len([f for f in folder.glob(\"*.png\")])\n\nclass_labels = [\"present\", \"absent\", \"external\"]\ncounts = [\n    count_pngs(SPECTROGRAM_PRESENT),\n    count_pngs(SPECTROGRAM_ABSENT),\n    count_pngs(SPECTROGRAM_EXTERNAL),\n]\n\nplt.figure(figsize=(6, 4))\nbars = plt.bar(class_labels, counts, color=[\"sienna\", \"peru\", \"gray\"], edgecolor=\"black\")\nplt.ylim(0, max(counts) * 1.1 if counts else 10)\nplt.title(\"Spectrogram Count per Class\")\nplt.ylabel(\"Images\")\nfor bar in bars:\n    y = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, y + max(1, y ** 0.5), int(y), ha=\"center\", va=\"bottom\")\nplt.show()\n\nprint(dict(zip(class_labels, counts)))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:28.078040Z",
     "iopub.execute_input": "2025-12-25T06:23:28.078307Z",
     "iopub.status.idle": "2025-12-25T06:23:28.204504Z",
     "shell.execute_reply.started": "2025-12-25T06:23:28.078286Z",
     "shell.execute_reply": "2025-12-25T06:23:28.203949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "{'present': 4000, 'absent': 2000, 'external': 2000}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "id": "b1438348",
   "cell_type": "code",
   "source": "IMG_SIZE = (128, 128)\nBASE_BATCH_SIZE = 32\nBATCH_SIZE = BASE_BATCH_SIZE  # Keep per-device batch size stable on Kaggle\nSEED = 42\n\nspectro_records = []\nfor class_dir in sorted(SPECTROGRAM_DIR.iterdir()):\n    if class_dir.is_dir():\n        label = class_dir.name\n        for img_path in class_dir.glob(\"*.png\"):\n            spectro_records.append({\"filepath\": str(img_path), \"label\": label})\n\nif not spectro_records:\n    raise RuntimeError(\"No spectrograms were generated; run preprocessing above first.\")\n\nspectro_df = pd.DataFrame(spectro_records)\nCLASS_NAMES = sorted(spectro_df[\"label\"].unique())\n\ntrain_df, temp_df = train_test_split(\n    spectro_df,\n    test_size=0.4,\n    stratify=spectro_df[\"label\"],\n    random_state=SEED\n)\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    stratify=temp_df[\"label\"],\n    random_state=SEED\n)\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    width_shift_range=0.05,\n    height_shift_range=0.05\n)\neval_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode=\"sparse\",\n    classes=CLASS_NAMES,\n    shuffle=True,\n    seed=SEED\n)\n\nval_gen = eval_datagen.flow_from_dataframe(\n    val_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode=\"sparse\",\n    classes=CLASS_NAMES,\n    shuffle=False,\n    seed=SEED\n)\n\ntest_gen = eval_datagen.flow_from_dataframe(\n    test_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode=\"sparse\",\n    classes=CLASS_NAMES,\n    shuffle=False,\n    seed=SEED\n)\n\nraw_class_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.array(CLASS_NAMES),\n    y=train_df[\"label\"]\n)\nCLASS_WEIGHTS = {\n    train_gen.class_indices[label]: weight for label, weight in zip(CLASS_NAMES, raw_class_weights)\n}\nprint(\"Class indices:\", train_gen.class_indices)\nprint(\"Class weights:\", CLASS_WEIGHTS)\n\nABSENT_CLASS_INDEX = train_gen.class_indices[\"absent\"]\n\nclass SparseClassRecall(tf.keras.metrics.Metric):\n    def __init__(self, class_id, name=\"sparse_class_recall\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.class_id = class_id\n        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n        self.false_negatives = self.add_weight(name=\"fn\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n        y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n        class_mask = tf.cast(tf.equal(y_true, self.class_id), self.dtype)\n        pred_mask = tf.cast(tf.equal(y_pred, self.class_id), self.dtype)\n        if sample_weight is None:\n            weights = tf.ones_like(class_mask)\n        else:\n            weights = tf.cast(tf.reshape(sample_weight, [-1]), self.dtype)\n            weights = tf.broadcast_to(weights, tf.shape(class_mask))\n        weighted_mask = class_mask * weights\n        tp = tf.reduce_sum(pred_mask * weighted_mask)\n        fn = tf.reduce_sum((1.0 - pred_mask) * weighted_mask)\n        self.true_positives.assign_add(tp)\n        self.false_negatives.assign_add(fn)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"class_id\": int(self.class_id)})\n        return config\n\n    def result(self):\n        return tf.math.divide_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n\n    def reset_states(self):\n        self.true_positives.assign(0.0)\n        self.false_negatives.assign(0.0)\n\ndef make_absent_recall(name=\"recall_absent\"):\n    return SparseClassRecall(class_id=ABSENT_CLASS_INDEX, name=name)\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:28.205295Z",
     "iopub.execute_input": "2025-12-25T06:23:28.205504Z",
     "iopub.status.idle": "2025-12-25T06:23:28.423997Z",
     "shell.execute_reply.started": "2025-12-25T06:23:28.205485Z",
     "shell.execute_reply": "2025-12-25T06:23:28.423453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 4800 validated image filenames belonging to 3 classes.\nFound 1600 validated image filenames belonging to 3 classes.\nFound 1600 validated image filenames belonging to 3 classes.\nClass indices: {'absent': 0, 'external': 1, 'present': 2}\nClass weights: {0: np.float64(1.3333333333333333), 1: np.float64(1.3333333333333333), 2: np.float64(0.6666666666666666)}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "id": "a233e93a",
   "cell_type": "code",
   "source": "from tensorflow.keras.callbacks import EarlyStopping\n\ndef build_baseline_model():\n    model = models.Sequential([\n        layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(),\n\n        layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(),\n\n        layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(),\n\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dropout(0.5),\n        layers.Dense(3, activation=\"softmax\"),\n    ])\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", make_absent_recall()]\n    )\n    return model\n\nwith strategy.scope():\n    baseline_model = build_baseline_model()\n\nbaseline_callbacks = [\n    EarlyStopping(monitor=\"val_recall_absent\", mode=\"max\", patience=3, restore_best_weights=True)\n]\n\nbaseline_history = baseline_model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=20,\n    class_weight=CLASS_WEIGHTS,\n    callbacks=baseline_callbacks\n)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:23:28.426062Z",
     "iopub.execute_input": "2025-12-25T06:23:28.426646Z",
     "iopub.status.idle": "2025-12-25T06:26:50.244737Z",
     "shell.execute_reply.started": "2025-12-25T06:23:28.426617Z",
     "shell.execute_reply": "2025-12-25T06:26:50.244176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "I0000 00:00:1766643808.910276      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1766643808.919170      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1/20\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1766643815.124289     129 service.cc:152] XLA service 0x7c887c0451c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1766643815.124323     129 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1766643815.124330     129 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1766643815.939226     129 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001b[1m  1/150\u001b[0m \u001b[37m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[1m23:53\u001b[0m 10s/step - accuracy: 0.2812 - loss: 1.2306 - recall_absent: 0.2222",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "I0000 00:00:1766643821.869223     129 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 146ms/step - accuracy: 0.7454 - loss: 0.6200 - recall_absent: 0.7550 - val_accuracy: 0.5000 - val_loss: 1.4907 - val_recall_absent: 0.0000e+00\nEpoch 2/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 140ms/step - accuracy: 0.8974 - loss: 0.2640 - recall_absent: 0.8974 - val_accuracy: 0.5100 - val_loss: 2.1924 - val_recall_absent: 0.0000e+00\nEpoch 3/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 137ms/step - accuracy: 0.9341 - loss: 0.1755 - recall_absent: 0.9215 - val_accuracy: 0.5931 - val_loss: 1.4677 - val_recall_absent: 0.0000e+00\nEpoch 4/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 139ms/step - accuracy: 0.9461 - loss: 0.1526 - recall_absent: 0.9376 - val_accuracy: 0.6494 - val_loss: 0.9180 - val_recall_absent: 0.0950\nEpoch 5/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 138ms/step - accuracy: 0.9597 - loss: 0.1117 - recall_absent: 0.9524 - val_accuracy: 0.6331 - val_loss: 1.3222 - val_recall_absent: 0.0050\nEpoch 6/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 138ms/step - accuracy: 0.9551 - loss: 0.1168 - recall_absent: 0.9424 - val_accuracy: 0.3162 - val_loss: 4.7943 - val_recall_absent: 0.2650\nEpoch 7/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 138ms/step - accuracy: 0.9692 - loss: 0.0917 - recall_absent: 0.9619 - val_accuracy: 0.3050 - val_loss: 3.9152 - val_recall_absent: 0.2200\nEpoch 8/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 139ms/step - accuracy: 0.9590 - loss: 0.1050 - recall_absent: 0.9529 - val_accuracy: 0.7156 - val_loss: 1.5098 - val_recall_absent: 0.0000e+00\nEpoch 9/20\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 138ms/step - accuracy: 0.9711 - loss: 0.0798 - recall_absent: 0.9645 - val_accuracy: 0.7319 - val_loss: 1.1412 - val_recall_absent: 0.0025\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 11
  },
  {
   "id": "75194ab4",
   "cell_type": "code",
   "source": "from tensorflow.keras.callbacks import EarlyStopping\nfrom pathlib import Path\n\ndef build_tunable_model(hp):\n    model = models.Sequential([\n        layers.Conv2D(\n            hp.Choice(\"conv1\", [32, 64]), 3,\n            activation=\"relu\", padding=\"same\",\n            input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n        ),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(),\n\n        layers.Conv2D(hp.Choice(\"conv2\", [64, 128]), 3, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(),\n\n        layers.Conv2D(hp.Choice(\"conv3\", [128, 256]), 3, activation=\"relu\", padding=\"same\"),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(),\n\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(hp.Int(\"dense_units\", 64, 128, step=32), activation=\"relu\"),\n        layers.Dropout(hp.Float(\"dropout\", 0.3, 0.6, step=0.1)),\n        layers.Dense(3, activation=\"softmax\"),\n    ])\n\n    model.compile(\n        optimizer=hp.Choice(\"optimizer\", [\"adam\", \"nadam\"]),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", make_absent_recall()]\n    )\n    return model\n\n\n# Strategy ONLY for tuner creation\nwith strategy.scope():\n    tuner = kt.Hyperband(\n        build_tunable_model,\n        objective=kt.Objective(\"val_recall_absent\", direction=\"max\"),\n        max_epochs=15,\n        factor=3,\n        directory=\"/kaggle/working/queenbee_tuning\",\n        project_name=\"queenbee_cnn\"\n    )\n\nstopper = EarlyStopping(\n    monitor=\"val_recall_absent\",\n    mode=\"max\",\n    patience=3,\n    restore_best_weights=True\n)\n\n# Search OUTSIDE strategy scope\ntuner.search(\n    train_gen,\n    validation_data=val_gen,\n    epochs=15,\n    class_weight=CLASS_WEIGHTS,\n    callbacks=[stopper]\n)\n\n# NO strategy scope here\nbest_model = tuner.get_best_models(num_models=1)[0]\n\nfine_tune_history = best_model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=15,\n    class_weight=CLASS_WEIGHTS,\n    callbacks=[stopper]\n)\n\n# Writable save path\nbest_model_path = Path(\"/kaggle/working/queenbee_final_tuned_model.keras\")\nbest_model.save(best_model_path)\n\nprint(\"Saved tuned model to\", best_model_path)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:26:50.245724Z",
     "iopub.execute_input": "2025-12-25T06:26:50.245995Z",
     "iopub.status.idle": "2025-12-25T06:29:51.067920Z",
     "shell.execute_reply.started": "2025-12-25T06:26:50.245974Z",
     "shell.execute_reply": "2025-12-25T06:29:51.067016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Reloading Tuner from /kaggle/working/queenbee_tuning/queenbee_cnn/tuner0.json\nEpoch 1/15\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2025-12-25 06:26:56.217121: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-25 06:26:56.390027: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 151ms/step - accuracy: 0.9026 - loss: 0.3379 - recall_absent: 0.8881 - val_accuracy: 0.3306 - val_loss: 5.2097 - val_recall_absent: 0.3150\nEpoch 2/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 143ms/step - accuracy: 0.9633 - loss: 0.0939 - recall_absent: 0.9617 - val_accuracy: 0.2581 - val_loss: 8.6065 - val_recall_absent: 0.0000e+00\nEpoch 3/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 143ms/step - accuracy: 0.9620 - loss: 0.0893 - recall_absent: 0.9617 - val_accuracy: 0.6356 - val_loss: 0.8437 - val_recall_absent: 0.9850\nEpoch 4/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 140ms/step - accuracy: 0.9686 - loss: 0.0762 - recall_absent: 0.9661 - val_accuracy: 0.8894 - val_loss: 0.3791 - val_recall_absent: 0.7725\nEpoch 5/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 141ms/step - accuracy: 0.9670 - loss: 0.0847 - recall_absent: 0.9479 - val_accuracy: 0.7550 - val_loss: 0.9985 - val_recall_absent: 1.0000\nEpoch 6/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 140ms/step - accuracy: 0.9788 - loss: 0.0522 - recall_absent: 0.9823 - val_accuracy: 0.7375 - val_loss: 2.6277 - val_recall_absent: 0.0100\nEpoch 7/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 140ms/step - accuracy: 0.9792 - loss: 0.0505 - recall_absent: 0.9819 - val_accuracy: 0.6881 - val_loss: 1.6765 - val_recall_absent: 0.0000e+00\nEpoch 8/15\n\u001b[1m150/150\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 140ms/step - accuracy: 0.9880 - loss: 0.0383 - recall_absent: 0.9863 - val_accuracy: 0.7350 - val_loss: 0.8766 - val_recall_absent: 0.0550\nSaved tuned model to /kaggle/working/queenbee_final_tuned_model.keras\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 12
  },
  {
   "id": "2e8f7167",
   "cell_type": "code",
   "source": "from tensorflow.keras.models import load_model\n\nmodel_for_eval = load_model(\n    best_model_path,\n    custom_objects={\"SparseClassRecall\": SparseClassRecall}\n)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:29:51.068966Z",
     "iopub.execute_input": "2025-12-25T06:29:51.069244Z",
     "iopub.status.idle": "2025-12-25T06:29:51.304155Z",
     "shell.execute_reply.started": "2025-12-25T06:29:51.069213Z",
     "shell.execute_reply": "2025-12-25T06:29:51.303563Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "id": "921011f7",
   "cell_type": "code",
   "source": "def run_inference(model, generator):\n    generator.reset()\n    y_prob = model.predict(generator, verbose=1)\n    y_true = generator.classes\n    return y_prob, y_true\n\n\ndef derive_thresholds(y_true, y_prob, class_names):\n    y_true_oh = tf.keras.utils.to_categorical(y_true, num_classes=len(class_names))\n    thresholds = {}\n    for idx, name in enumerate(class_names):\n        precision, recall, thresh = precision_recall_curve(y_true_oh[:, idx], y_prob[:, idx])\n        if thresh.size == 0:\n            thresholds[name] = 0.5\n            continue\n        f1 = 2 * precision * recall / np.clip(precision + recall, 1e-8, None)\n        best_idx = np.nanargmax(f1)\n        thresholds[name] = float(thresh[min(best_idx, len(thresh) - 1)])\n    return thresholds\n\n\ndef predict_with_thresholds(y_prob, class_names, thresholds):\n    calibrated = []\n    for row in y_prob:\n        chosen_idx = None\n        chosen_score = -1.0\n        for idx, name in enumerate(class_names):\n            threshold = thresholds.get(name, 0.5)\n            if row[idx] >= threshold and row[idx] > chosen_score:\n                chosen_idx = idx\n                chosen_score = row[idx]\n        if chosen_idx is None:\n            chosen_idx = int(np.argmax(row))\n        calibrated.append(chosen_idx)\n    return np.array(calibrated)\n\n\ndef summarize_metrics(y_true, y_pred, label):\n    return {\n        \"Mode\": label,\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Macro Precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"Macro Recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"Macro F1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n    }\n\nval_prob, val_true = run_inference(model_for_eval, val_gen)\nclass_names = list(test_gen.class_indices.keys())\nthresholds = derive_thresholds(val_true, val_prob, class_names)\nprint(\"Calibrated probability thresholds:\")\nfor name in class_names:\n    print(f\"  {name}: {thresholds[name]:.3f}\")\n\n\ntest_prob, test_true = run_inference(model_for_eval, test_gen)\ndefault_pred = np.argmax(test_prob, axis=1)\ncalibrated_pred = predict_with_thresholds(test_prob, class_names, thresholds)\n\nmetrics_table = pd.DataFrame([\n    summarize_metrics(test_true, default_pred, \"Argmax\"),\n    summarize_metrics(test_true, calibrated_pred, \"Calibrated\")\n])\ndisplay(metrics_table)\n\ncm = confusion_matrix(test_true, calibrated_pred)\nplt.figure(figsize=(5, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix (Calibrated)\")\nplt.show()\n\nprint(\"Calibrated classification report:\", classification_report(test_true, calibrated_pred, target_names=class_names, zero_division=0))\n\nroc_auc = roc_auc_score(\n    pd.get_dummies(test_true, drop_first=False).values,\n    test_prob,\n    average=\"macro\",\n    multi_class=\"ovr\"\n)\npr_auc = average_precision_score(\n    pd.get_dummies(test_true, drop_first=False).values,\n    test_prob,\n    average=\"macro\"\n)\nprint(f\"ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:29:51.305071Z",
     "iopub.execute_input": "2025-12-25T06:29:51.305338Z",
     "iopub.status.idle": "2025-12-25T06:29:55.355518Z",
     "shell.execute_reply.started": "2025-12-25T06:29:51.305312Z",
     "shell.execute_reply": "2025-12-25T06:29:55.354796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[1m50/50\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step\nCalibrated probability thresholds:\n  absent: 1.000\n  external: 0.028\n  present: 0.002\n\u001b[1m50/50\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         Mode  Accuracy  Macro Precision  Macro Recall  Macro F1\n0      Argmax  0.761875         0.833808      0.832500  0.786845\n1  Calibrated  0.944375         0.940172      0.955417  0.945805",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mode</th>\n      <th>Accuracy</th>\n      <th>Macro Precision</th>\n      <th>Macro Recall</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Argmax</td>\n      <td>0.761875</td>\n      <td>0.833808</td>\n      <td>0.832500</td>\n      <td>0.786845</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Calibrated</td>\n      <td>0.944375</td>\n      <td>0.940172</td>\n      <td>0.955417</td>\n      <td>0.945805</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Calibrated classification report:               precision    recall  f1-score   support\n\n      absent       0.84      0.97      0.90       400\n    external       1.00      0.98      0.99       400\n     present       0.98      0.91      0.94       800\n\n    accuracy                           0.94      1600\n   macro avg       0.94      0.96      0.95      1600\nweighted avg       0.95      0.94      0.95      1600\n\nROC-AUC: 0.9852 | PR-AUC: 0.9830\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "id": "8a188280",
   "cell_type": "code",
   "source": "SR = 22050\n\ndef audio_to_spectrogram_image(audio_path: Path):\n    y, sr = librosa.load(audio_path, sr=SR)\n    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n    S_dB = librosa.power_to_db(S, ref=np.max)\n\n    fig = plt.figure(figsize=(2, 2), dpi=64)\n    librosa.display.specshow(S_dB, sr=sr, cmap=\"magma\")\n    plt.axis(\"off\")\n\n    buf = io.BytesIO()\n    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close(fig)\n    buf.seek(0)\n\n    img = Image.open(buf).convert(\"RGB\").resize(IMG_SIZE)\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n    return img_array\n\ndef visualize_audio_prediction(audio_path: Path, model):\n    mel_input = audio_to_spectrogram_image(audio_path)\n    prediction = model.predict(mel_input)\n    class_names = list(test_gen.class_indices.keys())\n    pred_idx = int(np.argmax(prediction))\n    confidence = float(np.max(prediction))\n\n    y, sr = librosa.load(audio_path, sr=SR)\n    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n    mel_db = librosa.power_to_db(mel, ref=np.max)\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    times = np.linspace(0, len(y)/sr, len(y))\n    axes[0,0].plot(times, y)\n    axes[0,0].set_title(\"Waveform\")\n\n    img = axes[0,1].imshow(mel_db, aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n    axes[0,1].set_title(\"Mel Spectrogram\")\n    plt.colorbar(img, ax=axes[0,1], fraction=0.046, pad=0.04)\n\n    axes[1,0].bar(class_names, prediction[0], color=\"teal\")\n    axes[1,0].set_ylim(0, 1)\n    axes[1,0].set_title(\"Prediction Probabilities\")\n\n    axes[1,1].axis(\"off\")\n    axes[1,1].text(0.1, 0.5, f\"Predicted: {class_names[pred_idx]}\\nConfidence: {confidence:.2%}\", fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\"prediction\": class_names[pred_idx], \"confidence\": confidence}\n\nsample_audio = next(QUEEN_PRESENT_DIR.glob('*.wav'))\nvisualize_audio_prediction(sample_audio, model_for_eval)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:29:55.356476Z",
     "iopub.execute_input": "2025-12-25T06:29:55.356752Z",
     "iopub.status.idle": "2025-12-25T06:30:13.383486Z",
     "shell.execute_reply.started": "2025-12-25T06:29:55.356718Z",
     "shell.execute_reply": "2025-12-25T06:30:13.382906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979ms/step\n",
     "output_type": "stream"
    },
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'prediction': 'absent', 'confidence': 0.9853665828704834}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 15
  },
  {
   "id": "86e1ed93",
   "cell_type": "markdown",
   "source": "## Makueni Apiary Intelligence Pipeline",
   "metadata": {}
  },
  {
   "id": "6c6b0129",
   "cell_type": "code",
   "source": "DEFAULT_CENTER = (-1.8048, 37.62)\nENABLE_LEAFLET_WIDGETS = False  # Set True only if jupyter-leaflet widgets are installed.\n\ntry:\n    import ipywidgets as widgets\n    from ipyleaflet import Map, Marker, DrawControl, basemaps\nexcept Exception:\n    print(\"ipyleaflet not available; using default coordinates.\")\n    lat_widget = lon_widget = geometry_widget = None\nelse:\n    lat_widget = widgets.FloatText(value=DEFAULT_CENTER[0], description=\"Latitude\", step=0.0001)\n    lon_widget = widgets.FloatText(value=DEFAULT_CENTER[1], description=\"Longitude\", step=0.0001)\n    geometry_widget = widgets.Textarea(\n        value=\"\",\n        description=\"Geometry\",\n        placeholder=\"Draw a polygon/rectangle on the map.\",\n        layout=widgets.Layout(width=\"100%\", height=\"140px\"),\n        disabled=True,\n    )\n\n    leaflet_map = Map(center=DEFAULT_CENTER, zoom=8, basemap=basemaps.OpenStreetMap.Mapnik, scroll_wheel_zoom=True)\n    marker = Marker(location=DEFAULT_CENTER, draggable=True)\n    leaflet_map.add_layer(marker)\n\n    draw_control = DrawControl(\n        polygon={\"shapeOptions\": {\"color\": \"#2563eb\", \"weight\": 2, \"fillOpacity\": 0.2}},\n        rectangle={\"shapeOptions\": {\"color\": \"#f97316\", \"weight\": 2, \"fillOpacity\": 0.15}},\n        circle={},\n        circlemarker={},\n        polyline={},\n    )\n    leaflet_map.add_control(draw_control)\n\n    def _update_marker(change):\n        marker.location = (lat_widget.value, lon_widget.value)\n\n    lat_widget.observe(_update_marker, names=\"value\")\n    lon_widget.observe(_update_marker, names=\"value\")\n\n    display(widgets.HBox([lat_widget, lon_widget]))\n    display(geometry_widget)\n    display(leaflet_map)\n\nlat_widget_available = 'lat_widget' in globals() and lat_widget is not None\nlon_widget_available = 'lon_widget' in globals() and lon_widget is not None\n\nif lat_widget_available and lon_widget_available:\n    latitude = float(lat_widget.value)\n    longitude = float(lon_widget.value)\nelse:\n    latitude, longitude = DEFAULT_CENTER\n    print(\"Using default coordinates:\", DEFAULT_CENTER)\n\nselected_geometry_geojson = globals().get('selected_geometry_geojson')\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:30:13.384314Z",
     "iopub.execute_input": "2025-12-25T06:30:13.384853Z",
     "iopub.status.idle": "2025-12-25T06:30:13.625557Z",
     "shell.execute_reply.started": "2025-12-25T06:30:13.384813Z",
     "shell.execute_reply": "2025-12-25T06:30:13.624686Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatText(value=-1.8048, description='Latitude', step=0.0001), FloatText(value=37.62, descripti\u2026",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5262517f47ac4914ada2d1cd6e953524"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Textarea(value='', description='Geometry', disabled=True, layout=Layout(height='140px', width='100%'), placeho\u2026",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b92a1d3e3fd940f0a7f9c30d5ecbcc68"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map(center=[-1.8048, 37.62], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom\u2026",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0179490ccf5d4600a78de61bf0f9f8f1"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 16
  },
  {
   "id": "13f4ff43",
   "cell_type": "code",
   "source": "import ee\n\nraw_start_date = \"2008-01-01\"\nraw_end_date = \"2025-12-05\"\ntimezone = \"Africa/Nairobi\"\n\ndef normalize_date_string(d: str) -> dt.date:\n    parts = d.split(\"-\")\n    if len(parts) != 3:\n        raise ValueError(\"Date must be YYYY-MM-DD\")\n    y, m, day = [int(p) for p in parts]\n    m = max(1, min(12, m))\n    last_day = calendar.monthrange(y, m)[1]\n    day = max(1, min(last_day, day))\n    return dt.date(y, m, day)\n\nstart_date = normalize_date_string(raw_start_date)\nend_date = normalize_date_string(raw_end_date)\n\ntoday = dt.date.today()\napi_latest = dt.date(2025, 12, 20)\nmax_allowed = min(today, api_latest)\n\nif end_date > max_allowed:\n    print(f\"Clamping end_date {end_date} -> {max_allowed}\")\n    end_date = max_allowed\nif start_date > end_date:\n    raise ValueError(\"start_date must be before end_date\")\n\nprint(\"Using date range:\", start_date, \"\u2192\", end_date)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:30:13.626976Z",
     "iopub.execute_input": "2025-12-25T06:30:13.627449Z",
     "iopub.status.idle": "2025-12-25T06:30:13.879724Z",
     "shell.execute_reply.started": "2025-12-25T06:30:13.627417Z",
     "shell.execute_reply": "2025-12-25T06:30:13.879137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Using date range: 2008-01-01 \u2192 2025-12-05\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 17
  },
  {
   "id": "f651dd70",
   "cell_type": "code",
   "source": "ENABLE_REMOTE_CALLS = False  # Kaggle notebooks typically block outbound internet.\n\ndef split_date_range(start: dt.date, end: dt.date, max_days: int = 365):\n    chunks = []\n    current = start\n    while current <= end:\n        chunk_end = min(end, current + dt.timedelta(days=max_days - 1))\n        chunks.append((current, chunk_end))\n        current = chunk_end + dt.timedelta(days=1)\n    return chunks\n\ndef fetch_chunk(lat, lon, sdate: dt.date, edate: dt.date, timezone=\"Africa/Nairobi\", max_retries=3, backoff=2):\n    base = \"https://archive-api.open-meteo.com/v1/archive\"\n    daily_vars = \",\".join([\n        \"temperature_2m_max\",\n        \"temperature_2m_min\",\n        \"temperature_2m_mean\",\n        \"precipitation_sum\",\n        \"relative_humidity_2m_mean\",\n        \"wind_speed_10m_max\",\n        \"cloudcover_mean\"\n    ])\n    params = {\n        \"latitude\": lat,\n        \"longitude\": lon,\n        \"start_date\": sdate.strftime(\"%Y-%m-%d\"),\n        \"end_date\": edate.strftime(\"%Y-%m-%d\"),\n        \"daily\": daily_vars,\n        \"timezone\": timezone\n    }\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = requests.get(base, params=params, timeout=30)\n            resp.raise_for_status()\n            payload = resp.json()\n            if \"daily\" not in payload or \"time\" not in payload[\"daily\"]:\n                raise ValueError(\"API response missing expected fields.\")\n            return payload\n        except Exception as exc:\n            print(f\"Attempt {attempt} failed: {exc}\")\n            if attempt == max_retries:\n                raise\n            time.sleep(backoff ** attempt)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:33:52.549076Z",
     "iopub.execute_input": "2025-12-25T06:33:52.549682Z",
     "iopub.status.idle": "2025-12-25T06:33:52.556863Z",
     "shell.execute_reply.started": "2025-12-25T06:33:52.549653Z",
     "shell.execute_reply": "2025-12-25T06:33:52.556161Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "id": "b8b45a4d",
   "cell_type": "code",
   "source": "weather_csv = MAIN_DATA_DIR / \"makueni_weather_2008_2025.csv\"\nchunks = split_date_range(start_date, end_date, max_days=365)\n\nif ENABLE_REMOTE_CALLS:\n    dfs = []\n    for s, e in chunks:\n        payload = fetch_chunk(latitude, longitude, s, e, timezone=timezone)\n        daily = payload[\"daily\"]\n        df_chunk = pd.DataFrame({\n            \"date\": daily[\"time\"],\n            \"temp_max\": daily.get(\"temperature_2m_max\"),\n            \"temp_min\": daily.get(\"temperature_2m_min\"),\n            \"temp_mean\": daily.get(\"temperature_2m_mean\"),\n            \"humidity_mean\": daily.get(\"relative_humidity_2m_mean\"),\n            \"rainfall_mm\": daily.get(\"precipitation_sum\"),\n            \"wind_speed_max\": daily.get(\"wind_speed_10m_max\"),\n            \"cloud_cover_percent\": daily.get(\"cloudcover_mean\"),\n        })\n        dfs.append(df_chunk)\n        time.sleep(1)\n    weather_df = pd.concat(dfs, ignore_index=True)\n    weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n    weather_df.sort_values(\"date\", inplace=True)\n    weather_df.to_csv(weather_csv, index=False)\n    print(\"Fetched and saved weather CSV to\", weather_csv)\nelse:\n    if weather_csv.exists():\n        weather_df = pd.read_csv(weather_csv, parse_dates=[\"date\"])\n        print(f\"Loaded cached weather data from {weather_csv}\")\n    else:\n        raise FileNotFoundError(f\"{weather_csv} not found; enable ENABLE_REMOTE_CALLS to regenerate.\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:33:59.505530Z",
     "iopub.execute_input": "2025-12-25T06:33:59.506106Z",
     "iopub.status.idle": "2025-12-25T06:33:59.533559Z",
     "shell.execute_reply.started": "2025-12-25T06:33:59.506081Z",
     "shell.execute_reply": "2025-12-25T06:33:59.532862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loaded cached weather data from /kaggle/working/content/main-data/makueni_weather_2008_2025.csv\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "id": "0bd89f0c",
   "cell_type": "code",
   "source": "ndvi_csv = \"/kaggle/input/makueni-ndvi-2008-2025-csv/makueni_ndvi_2008_2025.csv\"\n\nif ENABLE_REMOTE_CALLS:\n    try:\n        ee.Initialize()\n    except Exception:\n        print(\"Authenticating with Earth Engine...\")\n        ee.Authenticate()\n        ee.Initialize()\n\n    point = ee.Geometry.Point([longitude, latitude])\n    modis = ee.ImageCollection(\"MODIS/061/MOD13Q1\").select(\"NDVI\").filterDate(start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")).filterBounds(point)\n\n    def extract_ndvi(image):\n        mean = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=point, scale=250).get(\"NDVI\")\n        date = image.date().format(\"YYYY-MM-dd\")\n        return ee.Feature(None, {\"date\": date, \"ndvi_mean\": mean})\n\n    ndvi_fc = modis.map(extract_ndvi).getInfo()\n    records = [f[\"properties\"] for f in ndvi_fc[\"features\"]]\n    ndvi_df = pd.DataFrame(records)\n    ndvi_df[\"date\"] = pd.to_datetime(ndvi_df[\"date\"])\n    ndvi_df[\"ndvi_mean\"] = ndvi_df[\"ndvi_mean\"].astype(float) / 10000\n    ndvi_df.to_csv(ndvi_csv, index=False)\n    print(\"Fetched NDVI and saved to\", ndvi_csv)\nelse:\n    if ndvi_csv:\n        ndvi_df = pd.read_csv(ndvi_csv, parse_dates=[\"date\"])\n        print(f\"Loaded cached NDVI data from {ndvi_csv}\")\n    else:\n        raise FileNotFoundError(f\"{ndvi_csv} not found; enable ENABLE_REMOTE_CALLS to regenerate.\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:40:40.108536Z",
     "iopub.execute_input": "2025-12-25T06:40:40.109243Z",
     "iopub.status.idle": "2025-12-25T06:40:40.129475Z",
     "shell.execute_reply.started": "2025-12-25T06:40:40.109215Z",
     "shell.execute_reply": "2025-12-25T06:40:40.128561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loaded cached NDVI data from /kaggle/input/makueni-ndvi-2008-2025-csv/makueni_ndvi_2008_2025.csv\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 30
  },
  {
   "id": "f84d3908",
   "cell_type": "code",
   "source": "df_weather = weather_df.copy()\ndf_ndvi = ndvi_df.copy()\n\ndf_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\ndf_ndvi[\"date\"] = pd.to_datetime(df_ndvi[\"date\"])\n\ndf_merged = pd.merge(df_weather, df_ndvi, on=\"date\", how=\"left\").sort_values(\"date\")\nweather_ndvi_path = MAIN_DATA_DIR / \"makueni_weather_ndvi_2008_2025.csv\"\ndf_merged.to_csv(weather_ndvi_path, index=False)\nprint(\"Merged weather+NDVI ->\", weather_ndvi_path)\ndf_merged.head()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:40:50.024565Z",
     "iopub.execute_input": "2025-12-25T06:40:50.024996Z",
     "iopub.status.idle": "2025-12-25T06:40:50.105558Z",
     "shell.execute_reply.started": "2025-12-25T06:40:50.024971Z",
     "shell.execute_reply": "2025-12-25T06:40:50.104761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Merged weather+NDVI -> /kaggle/working/content/main-data/makueni_weather_ndvi_2008_2025.csv\n",
     "output_type": "stream"
    },
    {
     "execution_count": 31,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        date  temp_max  temp_min  temp_mean  humidity_mean  rainfall_mm  \\\n0 2008-01-01      24.9      16.4       20.3             74          1.2   \n1 2008-01-02      25.8      14.1       20.2             71          0.8   \n2 2008-01-03      27.2      15.2       21.3             65          0.0   \n3 2008-01-04      27.6      15.4       22.2             63          0.1   \n4 2008-01-05      27.3      15.2       21.0             75          2.9   \n\n   wind_speed_max  cloud_cover_percent  ndvi_mean  \n0            15.1                   53     0.6805  \n1            14.3                   19        NaN  \n2            12.8                   11        NaN  \n3            12.2                   26        NaN  \n4            13.1                   58        NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>temp_max</th>\n      <th>temp_min</th>\n      <th>temp_mean</th>\n      <th>humidity_mean</th>\n      <th>rainfall_mm</th>\n      <th>wind_speed_max</th>\n      <th>cloud_cover_percent</th>\n      <th>ndvi_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-01-01</td>\n      <td>24.9</td>\n      <td>16.4</td>\n      <td>20.3</td>\n      <td>74</td>\n      <td>1.2</td>\n      <td>15.1</td>\n      <td>53</td>\n      <td>0.6805</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-01-02</td>\n      <td>25.8</td>\n      <td>14.1</td>\n      <td>20.2</td>\n      <td>71</td>\n      <td>0.8</td>\n      <td>14.3</td>\n      <td>19</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-01-03</td>\n      <td>27.2</td>\n      <td>15.2</td>\n      <td>21.3</td>\n      <td>65</td>\n      <td>0.0</td>\n      <td>12.8</td>\n      <td>11</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008-01-04</td>\n      <td>27.6</td>\n      <td>15.4</td>\n      <td>22.2</td>\n      <td>63</td>\n      <td>0.1</td>\n      <td>12.2</td>\n      <td>26</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008-01-05</td>\n      <td>27.3</td>\n      <td>15.2</td>\n      <td>21.0</td>\n      <td>75</td>\n      <td>2.9</td>\n      <td>13.1</td>\n      <td>58</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 31
  },
  {
   "id": "27121437",
   "cell_type": "code",
   "source": "df_month = df_merged.set_index(\"date\").resample(\"ME\").agg({\n    \"rainfall_mm\": \"sum\",\n    \"temp_mean\": \"mean\",\n    \"humidity_mean\": \"mean\",\n    \"ndvi_mean\": \"mean\"\n}).reset_index()\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\naxes[0].plot(df_month[\"date\"], df_month[\"rainfall_mm\"], marker=\"o\")\naxes[0].set_title(\"Monthly Rainfall (mm)\")\n\naxes[1].plot(df_month[\"date\"], df_month[\"temp_mean\"], marker=\"o\", color=\"tomato\")\naxes[1].set_title(\"Monthly Mean Temperature (\u00b0C)\")\n\naxes[2].plot(df_month[\"date\"], df_month[\"ndvi_mean\"], marker=\"o\", color=\"green\")\naxes[2].set_title(\"Monthly NDVI Mean\")\n\nfor ax in axes:\n    ax.grid(True, alpha=0.3)\n    ax.set_ylabel(\"Value\")\n\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:40:55.184868Z",
     "iopub.execute_input": "2025-12-25T06:40:55.185160Z",
     "iopub.status.idle": "2025-12-25T06:40:55.518403Z",
     "shell.execute_reply.started": "2025-12-25T06:40:55.185137Z",
     "shell.execute_reply": "2025-12-25T06:40:55.517602Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "id": "8dea5e59",
   "cell_type": "code",
   "source": "hive_logs_path = MAIN_DATA_DIR / \"hive_logs_2008_2025.csv\"\n\nif hive_logs_path.exists():\n    hive_df = pd.read_csv(hive_logs_path, parse_dates=[\"date\"])\n    print(\"Loaded hive logs from\", hive_logs_path)\nelse:\n    print(\"Generating synthetic hive telemetry...\")\n    start_dt = dt.datetime(2008, 1, 1)\n    end_dt = dt.datetime(2025, 9, 30)\n    dates = pd.date_range(start=start_dt, end=end_dt, freq=\"7D\")\n\n    hive_ids = [\"Hive-A\", \"Hive-B\", \"Hive-C\", \"Hive-D\"]\n    data = []\n    rng = np.random.default_rng(42)\n    for hive in hive_ids:\n        queen_age = rng.integers(3, 20)\n        for date in dates:\n            honey_yield = max(0, rng.normal(12, 3))\n            varroa = np.clip(rng.normal(8, 3), 0, 40)\n            hive_weight = rng.normal(45, 5)\n            brood_area = np.clip(rng.normal(800, 150), 100, 1200)\n            stress_event = rng.choice([\"none\", \"ants\", \"drought\"], p=[0.85, 0.1, 0.05])\n            data.append({\n                \"date\": date,\n                \"hive_id\": hive,\n                \"honey_yield_kg\": honey_yield,\n                \"varroa_pct\": varroa,\n                \"hive_weight_kg\": hive_weight,\n                \"brood_area_cm2\": brood_area,\n                \"stress_event\": stress_event,\n                \"queen_age_months\": queen_age\n            })\n    hive_df = pd.DataFrame(data)\n    hive_df.to_csv(hive_logs_path, index=False)\n    print(\"Synthetic hive logs saved to\", hive_logs_path)\n\nhive_df.head()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:41:05.555663Z",
     "iopub.execute_input": "2025-12-25T06:41:05.556446Z",
     "iopub.status.idle": "2025-12-25T06:41:05.774291Z",
     "shell.execute_reply.started": "2025-12-25T06:41:05.556417Z",
     "shell.execute_reply": "2025-12-25T06:41:05.773509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Generating synthetic hive telemetry...\nSynthetic hive logs saved to /kaggle/working/content/main-data/hive_logs_2008_2025.csv\n",
     "output_type": "stream"
    },
    {
     "execution_count": 33,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        date hive_id  honey_yield_kg  varroa_pct  hive_weight_kg  \\\n0 2008-01-01  Hive-A        8.880048   10.251354       49.702824   \n1 2008-01-08  Hive-A       12.383521    7.051272       44.915994   \n2 2008-01-15  Hive-A       14.333376    8.198092       50.636206   \n3 2008-01-22  Hive-A       13.106252    5.123352       49.392252   \n4 2008-01-29  Hive-A        9.957211   11.667624       44.227353   \n\n   brood_area_cm2 stress_event  queen_age_months  \n0      507.344722      drought                 4  \n1      672.043411         none                 4  \n2      870.126401         none                 4  \n3      792.511113         none                 4  \n4      735.750827         none                 4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>hive_id</th>\n      <th>honey_yield_kg</th>\n      <th>varroa_pct</th>\n      <th>hive_weight_kg</th>\n      <th>brood_area_cm2</th>\n      <th>stress_event</th>\n      <th>queen_age_months</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-01-01</td>\n      <td>Hive-A</td>\n      <td>8.880048</td>\n      <td>10.251354</td>\n      <td>49.702824</td>\n      <td>507.344722</td>\n      <td>drought</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-01-08</td>\n      <td>Hive-A</td>\n      <td>12.383521</td>\n      <td>7.051272</td>\n      <td>44.915994</td>\n      <td>672.043411</td>\n      <td>none</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-01-15</td>\n      <td>Hive-A</td>\n      <td>14.333376</td>\n      <td>8.198092</td>\n      <td>50.636206</td>\n      <td>870.126401</td>\n      <td>none</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008-01-22</td>\n      <td>Hive-A</td>\n      <td>13.106252</td>\n      <td>5.123352</td>\n      <td>49.392252</td>\n      <td>792.511113</td>\n      <td>none</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008-01-29</td>\n      <td>Hive-A</td>\n      <td>9.957211</td>\n      <td>11.667624</td>\n      <td>44.227353</td>\n      <td>735.750827</td>\n      <td>none</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 33
  },
  {
   "id": "3c6f516c",
   "cell_type": "code",
   "source": "weather_full = pd.read_csv(weather_ndvi_path, parse_dates=[\"date\"])\nhive_df[\"date\"] = pd.to_datetime(hive_df[\"date\"])\nmerged = pd.merge(hive_df, weather_full, on=\"date\", how=\"left\")\nmerged_path = MAIN_DATA_DIR / \"merged_hive_weather_ndvi.csv\"\nmerged.to_csv(merged_path, index=False)\nprint(\"Merged hive + weather ->\", merged_path)\nmerged.head()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:41:13.126761Z",
     "iopub.execute_input": "2025-12-25T06:41:13.127285Z",
     "iopub.status.idle": "2025-12-25T06:41:13.216129Z",
     "shell.execute_reply.started": "2025-12-25T06:41:13.127261Z",
     "shell.execute_reply": "2025-12-25T06:41:13.215555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Merged hive + weather -> /kaggle/working/content/main-data/merged_hive_weather_ndvi.csv\n",
     "output_type": "stream"
    },
    {
     "execution_count": 34,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        date hive_id  honey_yield_kg  varroa_pct  hive_weight_kg  \\\n0 2008-01-01  Hive-A        8.880048   10.251354       49.702824   \n1 2008-01-08  Hive-A       12.383521    7.051272       44.915994   \n2 2008-01-15  Hive-A       14.333376    8.198092       50.636206   \n3 2008-01-22  Hive-A       13.106252    5.123352       49.392252   \n4 2008-01-29  Hive-A        9.957211   11.667624       44.227353   \n\n   brood_area_cm2 stress_event  queen_age_months  temp_max  temp_min  \\\n0      507.344722      drought                 4      24.9      16.4   \n1      672.043411         none                 4      28.1      14.9   \n2      870.126401         none                 4      26.0      18.0   \n3      792.511113         none                 4      25.2      17.3   \n4      735.750827         none                 4      26.2      16.6   \n\n   temp_mean  humidity_mean  rainfall_mm  wind_speed_max  cloud_cover_percent  \\\n0       20.3             74          1.2            15.1                   53   \n1       21.8             61          0.0            13.6                   36   \n2       21.3             73          1.1            12.9                   64   \n3       20.4             79          1.5            13.1                   80   \n4       21.1             68          0.6            15.5                   72   \n\n   ndvi_mean  \n0     0.6805  \n1        NaN  \n2        NaN  \n3        NaN  \n4        NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>hive_id</th>\n      <th>honey_yield_kg</th>\n      <th>varroa_pct</th>\n      <th>hive_weight_kg</th>\n      <th>brood_area_cm2</th>\n      <th>stress_event</th>\n      <th>queen_age_months</th>\n      <th>temp_max</th>\n      <th>temp_min</th>\n      <th>temp_mean</th>\n      <th>humidity_mean</th>\n      <th>rainfall_mm</th>\n      <th>wind_speed_max</th>\n      <th>cloud_cover_percent</th>\n      <th>ndvi_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-01-01</td>\n      <td>Hive-A</td>\n      <td>8.880048</td>\n      <td>10.251354</td>\n      <td>49.702824</td>\n      <td>507.344722</td>\n      <td>drought</td>\n      <td>4</td>\n      <td>24.9</td>\n      <td>16.4</td>\n      <td>20.3</td>\n      <td>74</td>\n      <td>1.2</td>\n      <td>15.1</td>\n      <td>53</td>\n      <td>0.6805</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-01-08</td>\n      <td>Hive-A</td>\n      <td>12.383521</td>\n      <td>7.051272</td>\n      <td>44.915994</td>\n      <td>672.043411</td>\n      <td>none</td>\n      <td>4</td>\n      <td>28.1</td>\n      <td>14.9</td>\n      <td>21.8</td>\n      <td>61</td>\n      <td>0.0</td>\n      <td>13.6</td>\n      <td>36</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-01-15</td>\n      <td>Hive-A</td>\n      <td>14.333376</td>\n      <td>8.198092</td>\n      <td>50.636206</td>\n      <td>870.126401</td>\n      <td>none</td>\n      <td>4</td>\n      <td>26.0</td>\n      <td>18.0</td>\n      <td>21.3</td>\n      <td>73</td>\n      <td>1.1</td>\n      <td>12.9</td>\n      <td>64</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008-01-22</td>\n      <td>Hive-A</td>\n      <td>13.106252</td>\n      <td>5.123352</td>\n      <td>49.392252</td>\n      <td>792.511113</td>\n      <td>none</td>\n      <td>4</td>\n      <td>25.2</td>\n      <td>17.3</td>\n      <td>20.4</td>\n      <td>79</td>\n      <td>1.5</td>\n      <td>13.1</td>\n      <td>80</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008-01-29</td>\n      <td>Hive-A</td>\n      <td>9.957211</td>\n      <td>11.667624</td>\n      <td>44.227353</td>\n      <td>735.750827</td>\n      <td>none</td>\n      <td>4</td>\n      <td>26.2</td>\n      <td>16.6</td>\n      <td>21.1</td>\n      <td>68</td>\n      <td>0.6</td>\n      <td>15.5</td>\n      <td>72</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 34
  },
  {
   "id": "82d3330c",
   "cell_type": "code",
   "source": "floral_data = {\n    \"date\": [\n        \"2025-01-15\",\"2025-02-15\",\"2025-03-15\",\"2025-04-15\",\n        \"2025-05-15\",\"2025-06-15\",\"2025-07-15\",\"2025-08-15\",\"2025-09-15\"\n    ],\n    \"major_flowers\": [\n        \"Acacia tortilis, Mango, Commiphora\",\n        \"Acacia tortilis, Acacia mellifera, Mango\",\n        \"Croton, Acacia mellifera\",\n        \"Croton, Melia volkensii\",\n        \"Citrus, Croton\",\n        \"Aloe, Citrus\",\n        \"Aloe, Pasture weeds\",\n        \"Eucalyptus, Pasture weeds\",\n        \"Eucalyptus camaldulensis\"\n    ],\n    \"nectar_flow_strength\": [\"High\",\"High\",\"Medium\",\"Medium\",\"Medium\",\"Medium\",\"Low\",\"Low-Medium\",\"High\"],\n    \"stress_risk\": [\"Low\",\"Low\",\"Medium\",\"Low\",\"Medium\",\"Medium\",\"High\",\"High\",\"Low\"],\n    \"pest_disease_notes\": [\n        \"Hive beetles active\",\n        \"Wax moth pressure\",\n        \"Varroa buildup\",\n        \"Chalkbrood risk\",\n        \"Nosema risk\",\n        \"Slow brood buildup\",\n        \"Ant invasions\",\n        \"Weak colony pests\",\n        \"Healthy buildup\"\n    ]\n}\nfloral_df = pd.DataFrame(floral_data)\nfloral_df[\"date\"] = pd.to_datetime(floral_df[\"date\"])\nfloral_path = MAIN_DATA_DIR / \"makueni_floral_calendar_2025.csv\"\nfloral_df.to_csv(floral_path, index=False)\nprint(\"Floral calendar saved to\", floral_path)\nfloral_df\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:41:19.176170Z",
     "iopub.execute_input": "2025-12-25T06:41:19.176442Z",
     "iopub.status.idle": "2025-12-25T06:41:19.191216Z",
     "shell.execute_reply.started": "2025-12-25T06:41:19.176420Z",
     "shell.execute_reply": "2025-12-25T06:41:19.190504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Floral calendar saved to /kaggle/working/content/main-data/makueni_floral_calendar_2025.csv\n",
     "output_type": "stream"
    },
    {
     "execution_count": 35,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        date                             major_flowers nectar_flow_strength  \\\n0 2025-01-15        Acacia tortilis, Mango, Commiphora                 High   \n1 2025-02-15  Acacia tortilis, Acacia mellifera, Mango                 High   \n2 2025-03-15                  Croton, Acacia mellifera               Medium   \n3 2025-04-15                   Croton, Melia volkensii               Medium   \n4 2025-05-15                            Citrus, Croton               Medium   \n5 2025-06-15                              Aloe, Citrus               Medium   \n6 2025-07-15                       Aloe, Pasture weeds                  Low   \n7 2025-08-15                 Eucalyptus, Pasture weeds           Low-Medium   \n8 2025-09-15                  Eucalyptus camaldulensis                 High   \n\n  stress_risk   pest_disease_notes  \n0         Low  Hive beetles active  \n1         Low    Wax moth pressure  \n2      Medium       Varroa buildup  \n3         Low      Chalkbrood risk  \n4      Medium          Nosema risk  \n5      Medium   Slow brood buildup  \n6        High        Ant invasions  \n7        High    Weak colony pests  \n8         Low      Healthy buildup  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>major_flowers</th>\n      <th>nectar_flow_strength</th>\n      <th>stress_risk</th>\n      <th>pest_disease_notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-01-15</td>\n      <td>Acacia tortilis, Mango, Commiphora</td>\n      <td>High</td>\n      <td>Low</td>\n      <td>Hive beetles active</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-02-15</td>\n      <td>Acacia tortilis, Acacia mellifera, Mango</td>\n      <td>High</td>\n      <td>Low</td>\n      <td>Wax moth pressure</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-03-15</td>\n      <td>Croton, Acacia mellifera</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Varroa buildup</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-04-15</td>\n      <td>Croton, Melia volkensii</td>\n      <td>Medium</td>\n      <td>Low</td>\n      <td>Chalkbrood risk</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-05-15</td>\n      <td>Citrus, Croton</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Nosema risk</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2025-06-15</td>\n      <td>Aloe, Citrus</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Slow brood buildup</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2025-07-15</td>\n      <td>Aloe, Pasture weeds</td>\n      <td>Low</td>\n      <td>High</td>\n      <td>Ant invasions</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2025-08-15</td>\n      <td>Eucalyptus, Pasture weeds</td>\n      <td>Low-Medium</td>\n      <td>High</td>\n      <td>Weak colony pests</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2025-09-15</td>\n      <td>Eucalyptus camaldulensis</td>\n      <td>High</td>\n      <td>Low</td>\n      <td>Healthy buildup</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 35
  },
  {
   "id": "5f5ec06d",
   "cell_type": "code",
   "source": "merged_full = merged.merge(floral_df, on=\"date\", how=\"left\")\nmerged_full_path = MAIN_DATA_DIR / \"merged_hive_weather_floral_2025.csv\"\nmerged_full.to_csv(merged_full_path, index=False)\nprint(\"Merged hive/weather/floral ->\", merged_full_path)\nmerged_full.head()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:41:24.646405Z",
     "iopub.execute_input": "2025-12-25T06:41:24.646681Z",
     "iopub.status.idle": "2025-12-25T06:41:24.714644Z",
     "shell.execute_reply.started": "2025-12-25T06:41:24.646658Z",
     "shell.execute_reply": "2025-12-25T06:41:24.713994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Merged hive/weather/floral -> /kaggle/working/content/main-data/merged_hive_weather_floral_2025.csv\n",
     "output_type": "stream"
    },
    {
     "execution_count": 36,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        date hive_id  honey_yield_kg  varroa_pct  hive_weight_kg  \\\n0 2008-01-01  Hive-A        8.880048   10.251354       49.702824   \n1 2008-01-08  Hive-A       12.383521    7.051272       44.915994   \n2 2008-01-15  Hive-A       14.333376    8.198092       50.636206   \n3 2008-01-22  Hive-A       13.106252    5.123352       49.392252   \n4 2008-01-29  Hive-A        9.957211   11.667624       44.227353   \n\n   brood_area_cm2 stress_event  queen_age_months  temp_max  temp_min  \\\n0      507.344722      drought                 4      24.9      16.4   \n1      672.043411         none                 4      28.1      14.9   \n2      870.126401         none                 4      26.0      18.0   \n3      792.511113         none                 4      25.2      17.3   \n4      735.750827         none                 4      26.2      16.6   \n\n   temp_mean  humidity_mean  rainfall_mm  wind_speed_max  cloud_cover_percent  \\\n0       20.3             74          1.2            15.1                   53   \n1       21.8             61          0.0            13.6                   36   \n2       21.3             73          1.1            12.9                   64   \n3       20.4             79          1.5            13.1                   80   \n4       21.1             68          0.6            15.5                   72   \n\n   ndvi_mean major_flowers nectar_flow_strength stress_risk pest_disease_notes  \n0     0.6805           NaN                  NaN         NaN                NaN  \n1        NaN           NaN                  NaN         NaN                NaN  \n2        NaN           NaN                  NaN         NaN                NaN  \n3        NaN           NaN                  NaN         NaN                NaN  \n4        NaN           NaN                  NaN         NaN                NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>hive_id</th>\n      <th>honey_yield_kg</th>\n      <th>varroa_pct</th>\n      <th>hive_weight_kg</th>\n      <th>brood_area_cm2</th>\n      <th>stress_event</th>\n      <th>queen_age_months</th>\n      <th>temp_max</th>\n      <th>temp_min</th>\n      <th>temp_mean</th>\n      <th>humidity_mean</th>\n      <th>rainfall_mm</th>\n      <th>wind_speed_max</th>\n      <th>cloud_cover_percent</th>\n      <th>ndvi_mean</th>\n      <th>major_flowers</th>\n      <th>nectar_flow_strength</th>\n      <th>stress_risk</th>\n      <th>pest_disease_notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-01-01</td>\n      <td>Hive-A</td>\n      <td>8.880048</td>\n      <td>10.251354</td>\n      <td>49.702824</td>\n      <td>507.344722</td>\n      <td>drought</td>\n      <td>4</td>\n      <td>24.9</td>\n      <td>16.4</td>\n      <td>20.3</td>\n      <td>74</td>\n      <td>1.2</td>\n      <td>15.1</td>\n      <td>53</td>\n      <td>0.6805</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-01-08</td>\n      <td>Hive-A</td>\n      <td>12.383521</td>\n      <td>7.051272</td>\n      <td>44.915994</td>\n      <td>672.043411</td>\n      <td>none</td>\n      <td>4</td>\n      <td>28.1</td>\n      <td>14.9</td>\n      <td>21.8</td>\n      <td>61</td>\n      <td>0.0</td>\n      <td>13.6</td>\n      <td>36</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-01-15</td>\n      <td>Hive-A</td>\n      <td>14.333376</td>\n      <td>8.198092</td>\n      <td>50.636206</td>\n      <td>870.126401</td>\n      <td>none</td>\n      <td>4</td>\n      <td>26.0</td>\n      <td>18.0</td>\n      <td>21.3</td>\n      <td>73</td>\n      <td>1.1</td>\n      <td>12.9</td>\n      <td>64</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2008-01-22</td>\n      <td>Hive-A</td>\n      <td>13.106252</td>\n      <td>5.123352</td>\n      <td>49.392252</td>\n      <td>792.511113</td>\n      <td>none</td>\n      <td>4</td>\n      <td>25.2</td>\n      <td>17.3</td>\n      <td>20.4</td>\n      <td>79</td>\n      <td>1.5</td>\n      <td>13.1</td>\n      <td>80</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2008-01-29</td>\n      <td>Hive-A</td>\n      <td>9.957211</td>\n      <td>11.667624</td>\n      <td>44.227353</td>\n      <td>735.750827</td>\n      <td>none</td>\n      <td>4</td>\n      <td>26.2</td>\n      <td>16.6</td>\n      <td>21.1</td>\n      <td>68</td>\n      <td>0.6</td>\n      <td>15.5</td>\n      <td>72</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 36
  },
  {
   "id": "43772c05",
   "cell_type": "code",
   "source": "model_df = merged_full.copy()\nmodel_df[\"stress_event\"] = model_df[\"stress_event\"].fillna(\"none\")\nmodel_df[\"stress_target\"] = (model_df[\"stress_event\"] != \"none\").astype(int)\nmodel_df[\"date\"] = pd.to_datetime(model_df[\"date\"])\nmodel_df[\"month\"] = model_df[\"date\"].dt.month\nmodel_df[\"year\"] = model_df[\"date\"].dt.year\nmodel_df[\"weekofyear\"] = model_df[\"date\"].dt.isocalendar().week.astype(int)\n\nrolling_features = [\"honey_yield_kg\", \"varroa_pct\", \"hive_weight_kg\", \"brood_area_cm2\"]\nfor feature in rolling_features:\n    if feature in model_df.columns:\n        model_df[f\"{feature}_rolling_mean\"] = (\n            model_df.groupby(\"hive_id\")[feature]\n            .transform(lambda s: s.rolling(window=4, min_periods=1).mean())\n        )\n        model_df[f\"{feature}_rolling_std\"] = (\n            model_df.groupby(\"hive_id\")[feature]\n            .transform(lambda s: s.rolling(window=4, min_periods=1).std())\n        )\n\nnumeric_cols = model_df.select_dtypes(include=[np.number]).columns\nexclude_cols = {\"stress_target\"}\nfeature_cols = [col for col in numeric_cols if col not in exclude_cols]\nX = model_df[feature_cols].copy()\nX = X.dropna(axis=1, how=\"all\")\nfeature_cols = list(X.columns)\n\nimputer = SimpleImputer(strategy=\"median\")\nX = pd.DataFrame(imputer.fit_transform(X), columns=feature_cols, index=model_df.index)\ny = model_df[\"stress_target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\nclass_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nhb_model = HistGradientBoostingClassifier(\n    max_depth=6,\n    learning_rate=0.08,\n    max_iter=400,\n    class_weight=class_weight_dict\n)\n\nhb_model.fit(X_train, y_train)\ny_pred = hb_model.predict(X_test)\ny_prob = hb_model.predict_proba(X_test)[:, 1]\n\nprint(classification_report(y_test, y_pred))\nprint(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n\nRocCurveDisplay.from_predictions(y_test, y_prob)\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:41:32.346017Z",
     "iopub.execute_input": "2025-12-25T06:41:32.346495Z",
     "iopub.status.idle": "2025-12-25T06:41:33.383049Z",
     "shell.execute_reply.started": "2025-12-25T06:41:32.346470Z",
     "shell.execute_reply": "2025-12-25T06:41:33.382345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "              precision    recall  f1-score   support\n\n           0       0.84      0.95      0.89       625\n           1       0.14      0.04      0.06       117\n\n    accuracy                           0.81       742\n   macro avg       0.49      0.50      0.48       742\nweighted avg       0.73      0.81      0.76       742\n\nROC-AUC: 0.4957128205128205\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 37
  },
  {
   "id": "38cc1701",
   "cell_type": "code",
   "source": [
    "WINDOW_SIZE = 12\n",
    "feature_columns = [col for col in feature_cols if col in model_df.columns]\n",
    "sequence_features = model_df[feature_columns].fillna(model_df[feature_columns].median()).copy()\n",
    "sequence_targets = model_df['stress_target'].values\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "metadata = []\n",
    "for hive_id, group in model_df.groupby('hive_id'):\n",
    "    group = group.sort_values('date')\n",
    "    features = group[feature_columns].fillna(group[feature_columns].median()).values\n",
    "    targets = group['stress_target'].values\n",
    "    dates = group['date'].values\n",
    "    if len(group) <= WINDOW_SIZE:\n",
    "        continue\n",
    "    for idx in range(WINDOW_SIZE, len(group)):\n",
    "        window = features[idx-WINDOW_SIZE:idx]\n",
    "        X_sequences.append(window)\n",
    "        y_sequences.append(targets[idx])\n",
    "        metadata.append({\"hive_id\": hive_id, \"date\": dates[idx]})\n",
    "X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "y_sequences = np.array(y_sequences, dtype=np.float32)\n",
    "print(f'Total sequences: {X_sequences.shape[0]} | window shape: {X_sequences.shape[1:]}')\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences\n",
    ")\n",
    "train_dataset = SequenceDataset(X_train_seq, y_train_seq)\n",
    "val_dataset = SequenceDataset(X_test_seq, y_test_seq)\n",
    "train_class_counts = np.bincount(y_train_seq.astype(int))\n",
    "train_class_counts = np.pad(train_class_counts, (0, max(0, 2 - len(train_class_counts))), constant_values=0)\n",
    "print(\"Train class distribution:\", {cls: int(count) for cls, count in enumerate(train_class_counts)})\n",
    "samples_weight = np.array([1.0 / max(train_class_counts[int(label)], 1) for label in y_train_seq])\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=torch.tensor(samples_weight, dtype=torch.double),\n",
    "    num_samples=len(samples_weight),\n",
    "    replacement=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "input_channels = X_sequences.shape[-1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class HiveCNN(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        reduced_steps = max(1, WINDOW_SIZE // 2)\n",
    "        self.gru = nn.GRU(128, 96, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(96 * 2 * reduced_steps, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        gru_out, _ = self.gru(x)\n",
    "        x = self.classifier(gru_out.reshape(gru_out.size(0), -1))\n",
    "        return x.squeeze(-1)\n",
    "model = HiveCNN(input_channels).to(device)\n",
    "pos_weight_value = float(max(1.0, (len(y_train_seq) - y_train_seq.sum()) / max(1.0, y_train_seq.sum())))\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_value, device=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    for batch_X, batch_y in loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        total_loss += loss.item() * batch_X.size(0)\n",
    "        preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "        targets.append(batch_y.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    return total_loss / len(loader.dataset), roc_auc_score(targets, preds)\n",
    "epochs = 50\n",
    "best_auc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "model_path = MAIN_DATA_DIR / \"hive_cnn_torch.pt\"\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_auc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_auc = run_epoch(val_loader, train=False)\n",
    "    print(f\"Epoch {epoch+1:02d}: train_loss={train_loss:.4f} AUC={train_auc:.3f} | val_loss={val_loss:.4f} AUC={val_auc:.3f}\")\n",
    "    if val_auc > best_auc + 1e-3:\n",
    "        best_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = []\n",
    "    labels = []\n",
    "    for batch_X, batch_y in val_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        logits.append(model(batch_X).cpu())\n",
    "        labels.append(batch_y)\n",
    "    logits = torch.cat(logits)\n",
    "    labels = torch.cat(labels)\n",
    "    probs = torch.sigmoid(logits).numpy()\n",
    "    labels_np = labels.numpy()\n",
    "precision, recall, thresholds = precision_recall_curve(labels_np, probs)\n",
    "f_scores = (2 * precision * recall) / np.clip(precision + recall, 1e-8, None)\n",
    "best_idx = np.argmax(f_scores)\n",
    "best_threshold = thresholds[max(best_idx - 1, 0)] if best_idx < len(thresholds) else 0.5\n",
    "preds = (probs >= best_threshold).astype(int)\n",
    "print(f\"Best threshold based on F1: {best_threshold:.3f}\")\n",
    "print(classification_report(labels_np, preds))\n",
    "print('Test ROC-AUC:', roc_auc_score(labels_np, probs))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(recall, precision)\n",
    "axes[0].set_title('Precision-Recall')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "RocCurveDisplay.from_predictions(labels_np, probs, ax=axes[1])\n",
    "axes[1].set_title('ROC Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Best model weights saved to', model_path)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-25T06:41:44.567507Z",
     "iopub.execute_input": "2025-12-25T06:41:44.567839Z",
     "iopub.status.idle": "2025-12-25T06:41:52.580505Z",
     "shell.execute_reply.started": "2025-12-25T06:41:44.567808Z",
     "shell.execute_reply": "2025-12-25T06:41:52.579852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Total sequences: 3660 | window shape: (12, 24)\nEpoch 01: train_loss=1.1756 AUC=0.499 | val_loss=1.1680 AUC=0.522\nEpoch 02: train_loss=1.1649 AUC=0.550 | val_loss=1.1680 AUC=0.524\nEpoch 03: train_loss=1.1559 AUC=0.583 | val_loss=1.1655 AUC=0.530\nEpoch 04: train_loss=1.1478 AUC=0.594 | val_loss=1.2068 AUC=0.486\nEpoch 05: train_loss=1.1380 AUC=0.613 | val_loss=1.2579 AUC=0.554\nEpoch 06: train_loss=1.1209 AUC=0.639 | val_loss=1.2074 AUC=0.546\nEpoch 07: train_loss=1.0989 AUC=0.668 | val_loss=1.2047 AUC=0.549\nEpoch 08: train_loss=1.0745 AUC=0.693 | val_loss=1.2058 AUC=0.555\nEpoch 09: train_loss=1.0452 AUC=0.717 | val_loss=1.3070 AUC=0.496\nEpoch 10: train_loss=1.0110 AUC=0.742 | val_loss=1.5827 AUC=0.529\nEpoch 11: train_loss=0.9870 AUC=0.758 | val_loss=1.2929 AUC=0.534\nEpoch 12: train_loss=0.9467 AUC=0.785 | val_loss=1.7462 AUC=0.551\nEpoch 13: train_loss=0.9236 AUC=0.797 | val_loss=1.5463 AUC=0.551\nEpoch 14: train_loss=0.8639 AUC=0.830 | val_loss=3.0155 AUC=0.546\nEpoch 15: train_loss=0.8106 AUC=0.851 | val_loss=1.7846 AUC=0.466\nEpoch 16: train_loss=0.7876 AUC=0.856 | val_loss=5.9390 AUC=0.518\nEarly stopping triggered.\nBest threshold based on F1: 0.569\n              precision    recall  f1-score   support\n\n         0.0       0.87      0.76      0.81       617\n         1.0       0.23      0.38      0.29       115\n\n    accuracy                           0.70       732\n   macro avg       0.55      0.57      0.55       732\nweighted avg       0.77      0.70      0.73       732\n\nTest ROC-AUC: 0.5549150870269889\nBest model weights saved to /kaggle/working/content/main-data/hive_cnn_torch.pt\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 38
  }
 ]
}